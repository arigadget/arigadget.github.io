{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Donkey\u00ae \u672c\u30b5\u30a4\u30c8 Donkey is a high level self driving library written in Python. It was developed with a focus on enabling fast experimentation and easy contribution. Build your own Donkey Donkey is the standard car that most people build first. The parts cost about $250 to $300 and take 2 hours to assemble. Here are the main steps to build your own car: Assemble hardware. Install software. Create Donkey App. Calibrate your car. Start driving. Train an autopilot. Experiment with simulator. Hello World. Donkeycar is designed to make adding new parts to your car easy. Here's an example car application that captures images from the camera and saves them. import donkey as dk #initialize the vehicle V = dk.Vehicle() #add a camera part cam = dk.parts.PiCamera() V.add(cam, outputs=['image'], threaded=True) #add tub part to record images tub = dk.parts.Tub(path='~/mycar/data', inputs=['image'], types=['image_array']) V.add(tub, inputs=inputs) #start the vehicle's drive loop V.start(max_loop_count=100) Installation How to install Why the name Donkey? The ultimate goal of this project is to build something useful. Donkey's were one of the first domesticated pack animals, they're notoriously stubborn, and they are kid safe. Until the car can navigate from one side of a city to the other, we'll hold off naming it after some celestial being.","title":"About Donkey&reg;"},{"location":"#about-donkey","text":"\u672c\u30b5\u30a4\u30c8 Donkey is a high level self driving library written in Python. It was developed with a focus on enabling fast experimentation and easy contribution.","title":"About Donkey&reg;"},{"location":"#build-your-own-donkey","text":"Donkey is the standard car that most people build first. The parts cost about $250 to $300 and take 2 hours to assemble. Here are the main steps to build your own car: Assemble hardware. Install software. Create Donkey App. Calibrate your car. Start driving. Train an autopilot. Experiment with simulator.","title":"Build your own Donkey"},{"location":"#hello-world","text":"Donkeycar is designed to make adding new parts to your car easy. Here's an example car application that captures images from the camera and saves them. import donkey as dk #initialize the vehicle V = dk.Vehicle() #add a camera part cam = dk.parts.PiCamera() V.add(cam, outputs=['image'], threaded=True) #add tub part to record images tub = dk.parts.Tub(path='~/mycar/data', inputs=['image'], types=['image_array']) V.add(tub, inputs=inputs) #start the vehicle's drive loop V.start(max_loop_count=100)","title":"Hello World."},{"location":"#installation","text":"How to install","title":"Installation"},{"location":"#why-the-name-donkey","text":"The ultimate goal of this project is to build something useful. Donkey's were one of the first domesticated pack animals, they're notoriously stubborn, and they are kid safe. Until the car can navigate from one side of a city to the other, we'll hold off naming it after some celestial being.","title":"Why the name Donkey?"},{"location":"contribute/","text":"Contribute to Donkey Donkey is an open source project to help accelerate the development of self driving autos. Guiding Development Principles Modularity : A self driving system is composed of standalone, independently configurable components that can be combined to make a car. Minimalism : Each component should be kept short (<100 lines of code). Each piece of code should be transparent upon first reading. No black magic, it slows the speed of innovation. Extensibility : New components should be simple to create by following a template. Python : Keep it simple. These guidelines are nearly copied from Keras , because they are so good Add a part Are you a hardware specialist that can write a donkey part wrapper for a GPS unit or a data scientist that can write an recursive neural net autopilot? If so please write a part so other people driving donkeys can use the part. Fix or report a bug If you find a problem with the code and you know how to fix it then please clone the repo, make your fix, and submit your pull request. Reply to issues Helping close or triage the issues is a good way to help. If You Need An Inspiration Search the code or docs for TODO to find places where you might be able to find a better solution. Improve the documentation You can fix grammar or provide clarity by clicking the the Edit on GitHub link in the top right corner.","title":"Contribute to Donkey"},{"location":"contribute/#contribute-to-donkey","text":"Donkey is an open source project to help accelerate the development of self driving autos.","title":"Contribute to Donkey"},{"location":"contribute/#guiding-development-principles","text":"Modularity : A self driving system is composed of standalone, independently configurable components that can be combined to make a car. Minimalism : Each component should be kept short (<100 lines of code). Each piece of code should be transparent upon first reading. No black magic, it slows the speed of innovation. Extensibility : New components should be simple to create by following a template. Python : Keep it simple. These guidelines are nearly copied from Keras , because they are so good","title":"Guiding Development Principles"},{"location":"contribute/#add-a-part","text":"Are you a hardware specialist that can write a donkey part wrapper for a GPS unit or a data scientist that can write an recursive neural net autopilot? If so please write a part so other people driving donkeys can use the part.","title":"Add a part"},{"location":"contribute/#fix-or-report-a-bug","text":"If you find a problem with the code and you know how to fix it then please clone the repo, make your fix, and submit your pull request.","title":"Fix or report a bug"},{"location":"contribute/#reply-to-issues","text":"Helping close or triage the issues is a good way to help.","title":"Reply to issues"},{"location":"contribute/#if-you-need-an-inspiration","text":"Search the code or docs for TODO to find places where you might be able to find a better solution.","title":"If You Need An Inspiration"},{"location":"contribute/#improve-the-documentation","text":"You can fix grammar or provide clarity by clicking the the Edit on GitHub link in the top right corner.","title":"Improve the documentation"},{"location":"faq/","text":"FAQ What types of RC cars work with the donkey platform? Most hobby grade RC cars will work fine with the electronics but you'll need to make your own baseplate and camera holder. To make sure the car will work with Donkey check theses things. it has a separate ESC and receiver. Some of the cheaper cars have these combined so it would require soldering to connect the Donkey motor controller to the ESC. The ESC uses three-wire connectors. This will make it easy to just plug into the Donkey hardware. Brushed motors are easier because they can go slower but brushless motors can work as well. For more information, see Roll Your Own . What car can I use if I'm not in the USA? The easiest thing to do would be to take your parts down to your local RC / hobby shop and check that the car you want works with the parts. Here are some parts people have said work in other countries. Australia: KAOS (functionally equivalent to the Exceed Magnet) China: HSP 94186 (functionally equivalent to the Exceed Magnet) Add your country to this list (click edit this in top left corner) How can I make my own track? You can use tape, ribbon or even rope. The most popular tracks are 4ft wide and have 2in white borders with a dashed yellow center line. The Oakland track is about 70 feet around the center line. Key race characteristics include: straightaways. left and right turns hairpin turn start/finish line. Will Donkey Work on different hardware? Yes. It's all python so you can run it on any system. Usually the hard part of porting Donkey will be getting the hardware working. Here are a couple systems that people have tried or talked about. NVIDA TX2 - This was implemented with a webcam and used a teensy to control the motor/servos. I2c control of PCA9685 works as well. Pi-Zero - Yes, Try following the steps for the PiB/B+. They should work for the PiZero. After a reboot, I don't see the (donkey) in front of the prompt, and I get python errors when I run If you used this disc setup guide above, you used conda to manage your virtual environment. You need to activate the donkey conda environment with: conda activate donkey optionally you can add that line to the last line of your ~/.bashrc to have it active each time you login. How to get latest Donkey source When donkey has changed you can get the latest source. You've installed it directly from the github repo, so getting latest is easy: cd donkeycar git pull origin master donkey createcar --path ~/mycar --overwrite","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#what-types-of-rc-cars-work-with-the-donkey-platform","text":"Most hobby grade RC cars will work fine with the electronics but you'll need to make your own baseplate and camera holder. To make sure the car will work with Donkey check theses things. it has a separate ESC and receiver. Some of the cheaper cars have these combined so it would require soldering to connect the Donkey motor controller to the ESC. The ESC uses three-wire connectors. This will make it easy to just plug into the Donkey hardware. Brushed motors are easier because they can go slower but brushless motors can work as well. For more information, see Roll Your Own .","title":"What types of RC cars work with the donkey platform?"},{"location":"faq/#what-car-can-i-use-if-im-not-in-the-usa","text":"The easiest thing to do would be to take your parts down to your local RC / hobby shop and check that the car you want works with the parts. Here are some parts people have said work in other countries. Australia: KAOS (functionally equivalent to the Exceed Magnet) China: HSP 94186 (functionally equivalent to the Exceed Magnet) Add your country to this list (click edit this in top left corner)","title":"What car can I use if I'm not in the USA?"},{"location":"faq/#how-can-i-make-my-own-track","text":"You can use tape, ribbon or even rope. The most popular tracks are 4ft wide and have 2in white borders with a dashed yellow center line. The Oakland track is about 70 feet around the center line. Key race characteristics include: straightaways. left and right turns hairpin turn start/finish line.","title":"How can I make my own track?"},{"location":"faq/#will-donkey-work-on-different-hardware","text":"Yes. It's all python so you can run it on any system. Usually the hard part of porting Donkey will be getting the hardware working. Here are a couple systems that people have tried or talked about. NVIDA TX2 - This was implemented with a webcam and used a teensy to control the motor/servos. I2c control of PCA9685 works as well. Pi-Zero - Yes, Try following the steps for the PiB/B+. They should work for the PiZero.","title":"Will Donkey Work on different hardware?"},{"location":"faq/#after-a-reboot-i-dont-see-the-donkey-in-front-of-the-prompt-and-i-get-python-errors-when-i-run","text":"If you used this disc setup guide above, you used conda to manage your virtual environment. You need to activate the donkey conda environment with: conda activate donkey optionally you can add that line to the last line of your ~/.bashrc to have it active each time you login.","title":"After a reboot, I don't see the (donkey) in front of the prompt, and I get python errors when I run"},{"location":"faq/#how-to-get-latest-donkey-source","text":"When donkey has changed you can get the latest source. You've installed it directly from the github repo, so getting latest is easy: cd donkeycar git pull origin master donkey createcar --path ~/mycar --overwrite","title":"How to get latest Donkey source"},{"location":"legacy/","text":"Legacy This part of documentation it was left as reference to old original classic design, because it may still bring value for some Do-It-Yourself users. In the future this should be moved to another sections in the docs. Hardware If you purchased parts from the Donkey Car Store, skip to step 3. Step 1: Print Parts thingiverse I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down. Step 2: Clean up parts Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below: Step 3: Assemble Top plate and Roll Cage If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video Slide the nut into the slot in the side of the roll cage. This is not particularly easy. You may need to clean out the hole again and use a small screwdriver to push the screw in such that it lines up with the hole in the bottom of the roll cage. Once you have slid the nut in, you can attach the bottom plate. Once again, this may be tricky. I use the small screwdriver to push against the nut to keep it from spinning in the slot. Good news: you should never have to do this again. Step 4: Connect Servo Shield to Raspberry Pi You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground: Step 5: Attach Raspberry Pi to 3D Printed bottom plate Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro. Step 6: Attach Camera There are two versions of the donkey chassis, the newer one does not have screws, the older one does. This includes instructions for both: Screwless Design The newer design is pretty simple, just slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Design with Screws Attaching the camera is a little tricky, the M2 screws can be screwed into the plastic but it is a little hard. I recommend drilling the holes out with a 1.5mm bit (1/16th bit in Imperial land) then pre threading them with the screws before putting the camera on. It is only necessary to put two screws in. Sometimes using the two top screw holes can result in a short. Put screws in the bottom two holes. Before using the car, remove the plastic film from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this. Step 7: Put it all together Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!! Step 7b: Attach Adapters (Desert Monster only) The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7 Software Congrats! Now to get your get your car moving, see the software instructions section.","title":"Legacy"},{"location":"legacy/#legacy","text":"This part of documentation it was left as reference to old original classic design, because it may still bring value for some Do-It-Yourself users. In the future this should be moved to another sections in the docs.","title":"Legacy"},{"location":"legacy/#hardware","text":"If you purchased parts from the Donkey Car Store, skip to step 3.","title":"Hardware"},{"location":"legacy/#step-1-print-parts","text":"thingiverse I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down.","title":"Step 1: Print Parts"},{"location":"legacy/#step-2-clean-up-parts","text":"Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below:","title":"Step 2: Clean up parts"},{"location":"legacy/#step-3-assemble-top-plate-and-roll-cage","text":"If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video Slide the nut into the slot in the side of the roll cage. This is not particularly easy. You may need to clean out the hole again and use a small screwdriver to push the screw in such that it lines up with the hole in the bottom of the roll cage. Once you have slid the nut in, you can attach the bottom plate. Once again, this may be tricky. I use the small screwdriver to push against the nut to keep it from spinning in the slot. Good news: you should never have to do this again.","title":"Step 3: Assemble Top plate and Roll Cage"},{"location":"legacy/#step-4-connect-servo-shield-to-raspberry-pi","text":"You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground:","title":"Step 4: Connect Servo Shield to Raspberry Pi"},{"location":"legacy/#step-5-attach-raspberry-pi-to-3d-printed-bottom-plate","text":"Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro.","title":"Step 5: Attach Raspberry Pi to 3D Printed bottom plate"},{"location":"legacy/#step-6-attach-camera","text":"There are two versions of the donkey chassis, the newer one does not have screws, the older one does. This includes instructions for both: Screwless Design The newer design is pretty simple, just slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Design with Screws Attaching the camera is a little tricky, the M2 screws can be screwed into the plastic but it is a little hard. I recommend drilling the holes out with a 1.5mm bit (1/16th bit in Imperial land) then pre threading them with the screws before putting the camera on. It is only necessary to put two screws in. Sometimes using the two top screw holes can result in a short. Put screws in the bottom two holes. Before using the car, remove the plastic film from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this.","title":"Step 6: Attach Camera"},{"location":"legacy/#step-7-put-it-all-together","text":"Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!!","title":"Step 7: Put it all together"},{"location":"legacy/#step-7b-attach-adapters-desert-monster-only","text":"The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7","title":"Step 7b: Attach Adapters (Desert Monster only)"},{"location":"legacy/#software","text":"Congrats! Now to get your get your car moving, see the software instructions section.","title":"Software"},{"location":"release/","text":"Release notes Notes on how to release donkey. Create a startup disk. Download the previous disk image and create the startup disk. Move disk to you pi. Pull the lastest donkeycar code. Make your changes. Move the disk back to your computer. Clean up : Remove your wi-fi password Change the host name to donkeypi Delete .gitconfig . Create the disk image from the SD card Run sudo gparted to see the size of the disk partitions. Resize the partitions to be as small as possible. Right click the partition to see the last sector of the partition. Run: bash sudo dd if=/dev/mmcblk0 of=~/donkey_2.5.0_pi3.img bs=512 count=<last sector> status=progress Zip the .img file and upload to Dropbox. Update the link in the instructions. Create a release Run the tests on computer and pi. pytest Update versions in __init__ and setup.py","title":"Release notes"},{"location":"release/#release-notes","text":"Notes on how to release donkey.","title":"Release notes"},{"location":"release/#create-a-startup-disk","text":"Download the previous disk image and create the startup disk. Move disk to you pi. Pull the lastest donkeycar code. Make your changes. Move the disk back to your computer. Clean up : Remove your wi-fi password Change the host name to donkeypi Delete .gitconfig . Create the disk image from the SD card Run sudo gparted to see the size of the disk partitions. Resize the partitions to be as small as possible. Right click the partition to see the last sector of the partition. Run: bash sudo dd if=/dev/mmcblk0 of=~/donkey_2.5.0_pi3.img bs=512 count=<last sector> status=progress Zip the .img file and upload to Dropbox. Update the link in the instructions.","title":"Create a startup disk."},{"location":"release/#create-a-release","text":"Run the tests on computer and pi. pytest Update versions in __init__ and setup.py","title":"Create a release"},{"location":"roll_your_own/","text":"Roll Your Own Car The Quick and Dirty Your car needs to be easy to control from a Raspberry Pi Your car needs to be not too large, because it will be too heavy and dangerous (and expensive) Your car needs to be not too small, because it needs to carry a certain minimum amount of equipment Your car needs to meet minimum performance standards in power and control for the model to make sense for it Your car needs to be smooth to control even at low speeds This generally means: Your car needs to have a speed controller for the motor (ESC) that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a steering servo that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a radio receiver that contains standard 100-mil (2.54 mm) pin headers for each of the ESC and the steering servo. Your car needs to be between 1/18th scale (smallest end) and 1/8th scale (largest end) if you want to race in the DIYRobocars race. Your car needs to either use a brushed motor, or a sensored brushless motor. Sensorless brushless motors are too rough at low speeds. If you buy a car with a brushless motor included it is invariably a sensorless brushless motor and will need to be replaced along with the ESC. Other options are perhaps possible, see the end of this document. Many car builders end up looking at \"integrated\" RC hobby cars, because they are typically cheaper. However, the reason these are cheaper, is that they will integrate many parts of electronics and mechanics into a single package, which means that we can't intersect the appropriate signals to control the car with a Raspberry Pi. In fact, the expected signals may not even exist at all in an integrated car. Here is an example of an integrated RX and ESC - typically these should be avoided: You also need to know some things about electronics, such as the difference between power rails and control signals, what the duration of a microsecond is, and how Volts, Amperes, Watts, Hours, Ohms, and other measurement units relate. Chassis build While there are lots of designs out there besides the Donkeycar, but two stand out and are worth mentioning specifically. Chilicorn rail This is a flexible mounting system developed by Markku.ai. More details at markku.ai . sCAD Files Doug LaRue, a long time community member has extensive designs for making your own chassis in sCAD. If you want to roll your own but are not comfortable with CAD this is a good place to start. Basic Donkeycar 1/28 scale car inverted donkey car Servo Specifics An RC servo is used for controlling the steering wheels of the car. This servo typically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire. Typically, the three wires are colored black-red-white, or brown-red-yellow, where the dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control. The control signal is RC-style PWM, where one pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. When this pulse is 1500 microseconds, the servo is centered; when the pulse is 1000 microseconds, the servo is turned all the way left (or right) and when the pulse is 2000 microseconds, the servo is turned all the way in the other direction. This is NOT the same kind of PWM that you would use to control the duty cycle of a motor, or the brightness of a LED. The power for the servo typically comes from the motor ESC, which has a BEC (Battery Eliminator Circuit) built in. ESC Specifics The role of the ESC is to take a RC PWM control signal (pulse between 1000 and 2000 microseconds) in, and use that to control the power to the motor so the motor spins with different amounts of power in forward or reverse. Again, 1500 microseconds typically means \"center\" which for the motor means \"dead stop.\" The battery typically connects straight to the ESC using thicker wiring than the simple control signals, because the motor draws many more amps than the control. The ESC then connects on to the motor with equally thick power wiring. The standard Donkey motor and ESC probably have a peak current of about 12A; a 1/8th scale RC car with powerful brushless motor can have a peak draw up to 200A! Additionally, the ESC typically contains a linear or switching voltage converter that outputs the power needed to control the steering servo; this is typically somewhere in the 4.8V to 6V range. Most BECs built into ESCs will not deliver more than about 1A of current, so it is not typically possible to power both the steering servo and the Raspberry Pi from the BEC. Receiver Specifics If you buy a \"kit car\" that is listed as \"needs a receiver,\" then you don't need to buy a receiver. The Raspberry Pi plus the PCA9685 board take the role of the receiver, outputting control signals to the car. Buying a \"kit car\" that comes with steering servo, motor, and ESC, but not with radio, is actually a great way to make sure that the car you build will have the right signalling, because any RC car with separate receiver will be designed for the appropriate PWM signals. If your car comes with a receiver, make sure it has the appropriate three-pin headers next to each other for steering servo and for ESC control. Some receivers may have additional three-pin headers for additional channels, which may be empty or may control fancy attachments like horns, lights, and so forth. There is a modification to the Donkey car which uses the RC radio to drive the car when collecting training data; this will give better control of the car than you typically get with a PlayStation controller, or cell phone. However, it also requires replacing the PCA9685 board with an external micro-controller, and changing the software of the Donkey to use it. Finally, some receivers can output, in addition to the PWM control signals, a serial data packet that contains the control signals. An example of such a receiver is the FS-i6B, which has 6 output channels for PWM signals, but can output 10 channels of data at 115,200 bps as serial data, which you can read with an external micro-controller, or perhaps even with the Raspberry Pi (requires re-configuration of the Pi boot loader, and custom modifications to the donkey software.) Batteries The Donkey comes with a Nickel Metal Hydride battery (NiMH) which is just enough to make its motor go, for a little bit of time (5-10 minutes) before needing a recharge. The specifications on this battery are 6 cells, 1100 mAh. Because NiHM batteries range from 0.9V to 1.35V with a \"nominal\" voltage of 1.2V, you can expect to see voltages in the 5.4V to 8.1V range. NiHM batteries have medium energy capacity per weight and volume. Thus, you can improve the runtime and performance of the Magnet car by upgrading to a Lithium Polymer battery (LiPo.) Typically, you will get a 2 cell battery (2S) and Lithium batteries have 3.2V to 4.2V per cell, so you will see voltages in the 6.4V to 8.4V range. Additionally, Lithium Polymer batteries generally have higher current capacity (amount of Amps the battery can deliver at one point while driving) as well as energy storage (number of Amp Hours the battery stores when fully charged) so it may also last longer. Note that the amount of charge a battery can hold (how long it runs) is measured in Ampere-hours (Ah), or milli-Ampere-hours (mAh), whereas the amount of current a battery can instantaneously deliver while driving is measured simply in Amperes. But to make things more confusing, Amperes are often re-calculated in terms of multiples of the energy content, divided by one hour; this ratio is often called \"C.\" Thus, a LiPo rated for 10C and 2000 mAh, can deliver 20 Amperes of current while driving. A NiHM rated for 5C and 1100 mAh can deliver 5.5 Amperes of current while driving. Batteries typically will deliver more than the C rating for very short amounts of time, but will heat up or build up internal resistance such that that is not something you can rely on for normal operation. For your custom car, be aware of the voltages needed for the ESC and motor of the car, and make sure to get a battery that matches in voltage. Smaller RC cars will come with NiMH for affordability, or 2S LiPo for power. Larger RC cars will use 3S (11.1V) or 4S (14.8V) or even 6S (22.2V) Lithium batteries, and thus need to have ESC and motor combinations to match. Finally, be sure to get a charger that matches your battery. If you have a LiPo battery, get a good Lithium battery charger, with a balancing plug that matches your battery. Never discharge a Lithium battery below 3.2V per cell; if you let it run dead, it will not want to be charged up to normal voltage again, and trying to do so may very well overheat the batter and light it on fire! See YouTube pictures of burning Teslas for what that can look like. Seriously, houses have burned down because people have tried to save $10 by re-charging a Lithium battery that they forgot to disconnect and it ran down too much. It's not worth it. Instead, get a battery alarm, that you plug into the battery balance connector, and it beeps when the battery has discharged so much that you should disconnect and recharge it. Physical Constraints Adding the additional battery and electronics for self-driving to a toy car will add more load than the car was initially designed for. For a large, 1/8th scale car, this may not be much of a problem. For a small car, 1/18th scale or below, the additional weight and top-heaviness will cause the car to not react well to the steering output, which may cause the self-driving model to be less able to control the car. If you use a car that's not the standard Magnet, at a minimum, you will have to figure out how to mount all the hardware securely. Just piling things on and hoping wiring will keep it in place will not work for things that actually drive and turn. Finding good mounting points, and making your own \"base plate\" with measurements from the car you have, is likely to be necessary. You can build this base plate using 3D printing, laser cutting, CNC milling, or even just drilling into a thin piece of plywood, but getting a good fit to your chassis is important, so don't rush it or try to cut corners. Doug LaRue also built a configurator in Thingiverse that enables people to easily make custom 3D printed plates. Other Options Yes, you can make a self-driving car out of your 1/5th scale Nitro Dragster. You will just have to learn even more about the different bits and pieces of the solution, and figure out all the necessary integration yourself. The control signals for a Nitro car are the same, so this might not even be hard. However, the indoors arenas used for Donkey Racing Meetups do not allow fuel-burning cars, only electric. Yes, you can make a self-driving car out of a cheap two-wheel chassis that uses a LM298 H-bridge with direct PWM control to \"tank steer\" two wheels. However, you will have to adapt the Donkey software to output the right steering controls, and you will additionally have to figure out how to wire up the H-bridge to the Pi in a way that makes sense to you; the PWM signals output by the PCA9685 board are the RC control kind, NOT the motor control kind! Also, most affordable two-wheel-drive robot chassis are not actually big enough, strong enough, and mechanically consistent enough to make for good Donkey Car candidates.","title":"Roll Your Own Car"},{"location":"roll_your_own/#roll-your-own-car","text":"","title":"Roll Your Own Car"},{"location":"roll_your_own/#the-quick-and-dirty","text":"Your car needs to be easy to control from a Raspberry Pi Your car needs to be not too large, because it will be too heavy and dangerous (and expensive) Your car needs to be not too small, because it needs to carry a certain minimum amount of equipment Your car needs to meet minimum performance standards in power and control for the model to make sense for it Your car needs to be smooth to control even at low speeds This generally means: Your car needs to have a speed controller for the motor (ESC) that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a steering servo that takes a standard RC 3-pin control signal (RC PWM style) Your car needs to have a radio receiver that contains standard 100-mil (2.54 mm) pin headers for each of the ESC and the steering servo. Your car needs to be between 1/18th scale (smallest end) and 1/8th scale (largest end) if you want to race in the DIYRobocars race. Your car needs to either use a brushed motor, or a sensored brushless motor. Sensorless brushless motors are too rough at low speeds. If you buy a car with a brushless motor included it is invariably a sensorless brushless motor and will need to be replaced along with the ESC. Other options are perhaps possible, see the end of this document. Many car builders end up looking at \"integrated\" RC hobby cars, because they are typically cheaper. However, the reason these are cheaper, is that they will integrate many parts of electronics and mechanics into a single package, which means that we can't intersect the appropriate signals to control the car with a Raspberry Pi. In fact, the expected signals may not even exist at all in an integrated car. Here is an example of an integrated RX and ESC - typically these should be avoided: You also need to know some things about electronics, such as the difference between power rails and control signals, what the duration of a microsecond is, and how Volts, Amperes, Watts, Hours, Ohms, and other measurement units relate.","title":"The Quick and Dirty"},{"location":"roll_your_own/#chassis-build","text":"While there are lots of designs out there besides the Donkeycar, but two stand out and are worth mentioning specifically.","title":"Chassis build"},{"location":"roll_your_own/#chilicorn-rail","text":"This is a flexible mounting system developed by Markku.ai. More details at markku.ai .","title":"Chilicorn rail"},{"location":"roll_your_own/#scad-files","text":"Doug LaRue, a long time community member has extensive designs for making your own chassis in sCAD. If you want to roll your own but are not comfortable with CAD this is a good place to start. Basic Donkeycar 1/28 scale car inverted donkey car","title":"sCAD Files"},{"location":"roll_your_own/#servo-specifics","text":"An RC servo is used for controlling the steering wheels of the car. This servo typically expects around 4.8V to 6V input on the power wire (varies by car) and a PWM control signal on the signal wire. Typically, the three wires are colored black-red-white, or brown-red-yellow, where the dark wire (black/brown) is ground, and the center wire (red) is power, and the light wire (white/yellow) is control. The control signal is RC-style PWM, where one pulse is sent 60 times a second, and the width of this pulse controls how left/right the servo turns. When this pulse is 1500 microseconds, the servo is centered; when the pulse is 1000 microseconds, the servo is turned all the way left (or right) and when the pulse is 2000 microseconds, the servo is turned all the way in the other direction. This is NOT the same kind of PWM that you would use to control the duty cycle of a motor, or the brightness of a LED. The power for the servo typically comes from the motor ESC, which has a BEC (Battery Eliminator Circuit) built in.","title":"Servo Specifics"},{"location":"roll_your_own/#esc-specifics","text":"The role of the ESC is to take a RC PWM control signal (pulse between 1000 and 2000 microseconds) in, and use that to control the power to the motor so the motor spins with different amounts of power in forward or reverse. Again, 1500 microseconds typically means \"center\" which for the motor means \"dead stop.\" The battery typically connects straight to the ESC using thicker wiring than the simple control signals, because the motor draws many more amps than the control. The ESC then connects on to the motor with equally thick power wiring. The standard Donkey motor and ESC probably have a peak current of about 12A; a 1/8th scale RC car with powerful brushless motor can have a peak draw up to 200A! Additionally, the ESC typically contains a linear or switching voltage converter that outputs the power needed to control the steering servo; this is typically somewhere in the 4.8V to 6V range. Most BECs built into ESCs will not deliver more than about 1A of current, so it is not typically possible to power both the steering servo and the Raspberry Pi from the BEC.","title":"ESC Specifics"},{"location":"roll_your_own/#receiver-specifics","text":"If you buy a \"kit car\" that is listed as \"needs a receiver,\" then you don't need to buy a receiver. The Raspberry Pi plus the PCA9685 board take the role of the receiver, outputting control signals to the car. Buying a \"kit car\" that comes with steering servo, motor, and ESC, but not with radio, is actually a great way to make sure that the car you build will have the right signalling, because any RC car with separate receiver will be designed for the appropriate PWM signals. If your car comes with a receiver, make sure it has the appropriate three-pin headers next to each other for steering servo and for ESC control. Some receivers may have additional three-pin headers for additional channels, which may be empty or may control fancy attachments like horns, lights, and so forth. There is a modification to the Donkey car which uses the RC radio to drive the car when collecting training data; this will give better control of the car than you typically get with a PlayStation controller, or cell phone. However, it also requires replacing the PCA9685 board with an external micro-controller, and changing the software of the Donkey to use it. Finally, some receivers can output, in addition to the PWM control signals, a serial data packet that contains the control signals. An example of such a receiver is the FS-i6B, which has 6 output channels for PWM signals, but can output 10 channels of data at 115,200 bps as serial data, which you can read with an external micro-controller, or perhaps even with the Raspberry Pi (requires re-configuration of the Pi boot loader, and custom modifications to the donkey software.)","title":"Receiver Specifics"},{"location":"roll_your_own/#batteries","text":"The Donkey comes with a Nickel Metal Hydride battery (NiMH) which is just enough to make its motor go, for a little bit of time (5-10 minutes) before needing a recharge. The specifications on this battery are 6 cells, 1100 mAh. Because NiHM batteries range from 0.9V to 1.35V with a \"nominal\" voltage of 1.2V, you can expect to see voltages in the 5.4V to 8.1V range. NiHM batteries have medium energy capacity per weight and volume. Thus, you can improve the runtime and performance of the Magnet car by upgrading to a Lithium Polymer battery (LiPo.) Typically, you will get a 2 cell battery (2S) and Lithium batteries have 3.2V to 4.2V per cell, so you will see voltages in the 6.4V to 8.4V range. Additionally, Lithium Polymer batteries generally have higher current capacity (amount of Amps the battery can deliver at one point while driving) as well as energy storage (number of Amp Hours the battery stores when fully charged) so it may also last longer. Note that the amount of charge a battery can hold (how long it runs) is measured in Ampere-hours (Ah), or milli-Ampere-hours (mAh), whereas the amount of current a battery can instantaneously deliver while driving is measured simply in Amperes. But to make things more confusing, Amperes are often re-calculated in terms of multiples of the energy content, divided by one hour; this ratio is often called \"C.\" Thus, a LiPo rated for 10C and 2000 mAh, can deliver 20 Amperes of current while driving. A NiHM rated for 5C and 1100 mAh can deliver 5.5 Amperes of current while driving. Batteries typically will deliver more than the C rating for very short amounts of time, but will heat up or build up internal resistance such that that is not something you can rely on for normal operation. For your custom car, be aware of the voltages needed for the ESC and motor of the car, and make sure to get a battery that matches in voltage. Smaller RC cars will come with NiMH for affordability, or 2S LiPo for power. Larger RC cars will use 3S (11.1V) or 4S (14.8V) or even 6S (22.2V) Lithium batteries, and thus need to have ESC and motor combinations to match. Finally, be sure to get a charger that matches your battery. If you have a LiPo battery, get a good Lithium battery charger, with a balancing plug that matches your battery. Never discharge a Lithium battery below 3.2V per cell; if you let it run dead, it will not want to be charged up to normal voltage again, and trying to do so may very well overheat the batter and light it on fire! See YouTube pictures of burning Teslas for what that can look like. Seriously, houses have burned down because people have tried to save $10 by re-charging a Lithium battery that they forgot to disconnect and it ran down too much. It's not worth it. Instead, get a battery alarm, that you plug into the battery balance connector, and it beeps when the battery has discharged so much that you should disconnect and recharge it.","title":"Batteries"},{"location":"roll_your_own/#physical-constraints","text":"Adding the additional battery and electronics for self-driving to a toy car will add more load than the car was initially designed for. For a large, 1/8th scale car, this may not be much of a problem. For a small car, 1/18th scale or below, the additional weight and top-heaviness will cause the car to not react well to the steering output, which may cause the self-driving model to be less able to control the car. If you use a car that's not the standard Magnet, at a minimum, you will have to figure out how to mount all the hardware securely. Just piling things on and hoping wiring will keep it in place will not work for things that actually drive and turn. Finding good mounting points, and making your own \"base plate\" with measurements from the car you have, is likely to be necessary. You can build this base plate using 3D printing, laser cutting, CNC milling, or even just drilling into a thin piece of plywood, but getting a good fit to your chassis is important, so don't rush it or try to cut corners. Doug LaRue also built a configurator in Thingiverse that enables people to easily make custom 3D printed plates.","title":"Physical Constraints"},{"location":"roll_your_own/#other-options","text":"Yes, you can make a self-driving car out of your 1/5th scale Nitro Dragster. You will just have to learn even more about the different bits and pieces of the solution, and figure out all the necessary integration yourself. The control signals for a Nitro car are the same, so this might not even be hard. However, the indoors arenas used for Donkey Racing Meetups do not allow fuel-burning cars, only electric. Yes, you can make a self-driving car out of a cheap two-wheel chassis that uses a LM298 H-bridge with direct PWM control to \"tank steer\" two wheels. However, you will have to adapt the Donkey software to output the right steering controls, and you will additionally have to figure out how to wire up the H-bridge to the Pi in a way that makes sense to you; the PWM signals output by the PCA9685 board are the RC control kind, NOT the motor control kind! Also, most affordable two-wheel-drive robot chassis are not actually big enough, strong enough, and mechanically consistent enough to make for good Donkey Car candidates.","title":"Other Options"},{"location":"supported_cars/","text":"Supported cars Magnet and HSP 94186 The magnet chassis was the first standard Donkey build. However in many cases it may not be available. Try searching for both the Magnet and HSP 94186 on ebay, banggood, ali express etc. The HSP 94186 is the same as the Magnet and will work. If you speak mandarin it is always available on Taobao. Taobao item offer Exceed Desert Monster, Short Course Truck, and Blaze The Desert Monster, SCT and Blaze are made by the same manufacturer as the Magnet and has the same motor and ESC. The chassis is slightly different so it requires an adapter and some extra hardware to work with the standard donkey platform. With the adapters the camera placement will be identical to the Magnet and should be able to share models. It is worth noting that the Desert Monster and SCT also has some nice characteristics including narrower, more road friendly tires and the Blaze has a slightly narrower stance which makes it less likely to hit things. To purchase one of these cars follow the following links: Exceed Desert Monster Blue , Red Exceed Short Course Truck Blue , Red Exceed Blaze Hyper Blue , Yellow To assemble one of these you will need some additional parts than the standard build, these can be purchased as a kit on the donkey store at: Purchase: Donkey Store Part Description Link Approximate Cost 3D printed Adapters Files: thingiverse.com/thing:2260575 $10 Chassis Clips Amazon $5 To assemble first remove the plastic cover and roll cage then unscrew the posts that hold up the cover and replace with the adapters. Visual instructions to follow. LaTrax Prerunner The LaTrax prerunner is a supported car and follows the same build instructions as the Desert Monster. However the adapters get screwed in as is shown in the photo below. Donkey Pro To build a donkey pro the following parts are Needed Part Description Link Approximate Cost Donkey Pro Plastics and base Thingiverse or Donkeystore $50 (8) M2.5 standoff (8) M2.5 Nylock nuts (8) M2.5x6mm socket head cap screws (4) M3x10 plastic self threading screw To assemble the Raspberry pi to the chassis this assembly picture should clarify how it fits together. Tamaya TT-01 (Advanced Build) The TT-01 is a new build that is a higher end version of the Donkey. This is an advanced build and requires existing RC skills or the desire to learn them - along with some willingness to trial and error. For first time builders we recommend the Magnet. That said, it has some pros and cons that people should be aware of, presented below. Pros: Better kinematics and traction on smooth surfaces - basically this means it will corner better Larger build area for adding other sensors. Globally available with several clones. Cons: Assembly required! - you will need to supply your own ESC, battery, servo, pinion gear and motor. Needs to run on a smooth surface like a driveway or parking lot. Larger size requires a larger 3D printer to print chassis, otherwise purchase at the Donkeystore. More expensive In addition to the standard donkey parts, Raspberry Pi etc, you will need to buy the following components. Part Description Link Approximate Cost TT-01 Clone Chassis eBay other TT01s may be used $130 ESC Hobbyking 10.60 Brushed Motor Hobbyking $5 Steering Servo Hobbyking $5 Battery Hobbyking or similar 2S 5000 mAh battery $21 Pinion Gear Amazon $7 TT01 Plastics Thingiverse or Donkeystore $50 Note: purchasing from Hobbyking is tricky. They can ship from multiple warehouses and it can be expensive and time consuming if shipping from one overseas. You may need to buy an alternate component if one of the items above are not available in your local warehouse. If You Want to Roll Your Own It's totally possible to diverge from the main Donkey build, and still have a car that drives well and is fun to work with. We've seen a large variety of cars in the various Donkey competitions around the world. However, when you want to diverge, there are several things you need to know, or you will not be successful. There are many cost and quality trade-offs where the lower cost options simply won't work. We've already worked hard to find the cheapest available options that will work, so you should not expect to choose other options to save money. Rolling your own is more about learning, experimentation, and going to new and uncharged places. To find out more about what you need, see Roll Your Own .","title":"Supported cars"},{"location":"supported_cars/#supported-cars","text":"","title":"Supported cars"},{"location":"supported_cars/#magnet-and-hsp-94186","text":"The magnet chassis was the first standard Donkey build. However in many cases it may not be available. Try searching for both the Magnet and HSP 94186 on ebay, banggood, ali express etc. The HSP 94186 is the same as the Magnet and will work. If you speak mandarin it is always available on Taobao. Taobao item offer","title":"Magnet and HSP 94186"},{"location":"supported_cars/#exceed-desert-monster-short-course-truck-and-blaze","text":"The Desert Monster, SCT and Blaze are made by the same manufacturer as the Magnet and has the same motor and ESC. The chassis is slightly different so it requires an adapter and some extra hardware to work with the standard donkey platform. With the adapters the camera placement will be identical to the Magnet and should be able to share models. It is worth noting that the Desert Monster and SCT also has some nice characteristics including narrower, more road friendly tires and the Blaze has a slightly narrower stance which makes it less likely to hit things. To purchase one of these cars follow the following links: Exceed Desert Monster Blue , Red Exceed Short Course Truck Blue , Red Exceed Blaze Hyper Blue , Yellow To assemble one of these you will need some additional parts than the standard build, these can be purchased as a kit on the donkey store at: Purchase: Donkey Store Part Description Link Approximate Cost 3D printed Adapters Files: thingiverse.com/thing:2260575 $10 Chassis Clips Amazon $5 To assemble first remove the plastic cover and roll cage then unscrew the posts that hold up the cover and replace with the adapters. Visual instructions to follow.","title":"Exceed Desert Monster, Short Course Truck, and Blaze"},{"location":"supported_cars/#latrax-prerunner","text":"The LaTrax prerunner is a supported car and follows the same build instructions as the Desert Monster. However the adapters get screwed in as is shown in the photo below.","title":"LaTrax Prerunner"},{"location":"supported_cars/#donkey-pro","text":"To build a donkey pro the following parts are Needed Part Description Link Approximate Cost Donkey Pro Plastics and base Thingiverse or Donkeystore $50 (8) M2.5 standoff (8) M2.5 Nylock nuts (8) M2.5x6mm socket head cap screws (4) M3x10 plastic self threading screw To assemble the Raspberry pi to the chassis this assembly picture should clarify how it fits together.","title":"Donkey Pro"},{"location":"supported_cars/#tamaya-tt-01-advanced-build","text":"The TT-01 is a new build that is a higher end version of the Donkey. This is an advanced build and requires existing RC skills or the desire to learn them - along with some willingness to trial and error. For first time builders we recommend the Magnet. That said, it has some pros and cons that people should be aware of, presented below. Pros: Better kinematics and traction on smooth surfaces - basically this means it will corner better Larger build area for adding other sensors. Globally available with several clones. Cons: Assembly required! - you will need to supply your own ESC, battery, servo, pinion gear and motor. Needs to run on a smooth surface like a driveway or parking lot. Larger size requires a larger 3D printer to print chassis, otherwise purchase at the Donkeystore. More expensive In addition to the standard donkey parts, Raspberry Pi etc, you will need to buy the following components. Part Description Link Approximate Cost TT-01 Clone Chassis eBay other TT01s may be used $130 ESC Hobbyking 10.60 Brushed Motor Hobbyking $5 Steering Servo Hobbyking $5 Battery Hobbyking or similar 2S 5000 mAh battery $21 Pinion Gear Amazon $7 TT01 Plastics Thingiverse or Donkeystore $50 Note: purchasing from Hobbyking is tricky. They can ship from multiple warehouses and it can be expensive and time consuming if shipping from one overseas. You may need to buy an alternate component if one of the items above are not available in your local warehouse.","title":"Tamaya TT-01 (Advanced Build)"},{"location":"supported_cars/#if-you-want-to-roll-your-own","text":"It's totally possible to diverge from the main Donkey build, and still have a car that drives well and is fun to work with. We've seen a large variety of cars in the various Donkey competitions around the world. However, when you want to diverge, there are several things you need to know, or you will not be successful. There are many cost and quality trade-offs where the lower cost options simply won't work. We've already worked hard to find the cheapest available options that will work, so you should not expect to choose other options to save money. Rolling your own is more about learning, experimentation, and going to new and uncharged places. To find out more about what you need, see Roll Your Own .","title":"If You Want to Roll Your Own"},{"location":"tests/","text":"Tests There is a limited test suite to ensure that the your changes to the code don't break something unintended. Run all the tests Look into the .travis.yml for a more detailed commands used for tests: install section is used to install required packages and setting up environment. script section is actually used to run test suite jobs section may contain another tasks related to other things like tests or deployments. Notice: in .travis.yml env var named TRAVIS_PYTHON_VERSION is populated from python section, such as TRAVIS_PYTHON_VERSION=3.6 . Please refer to the travis documentation for more details. Code Organization The test code is in tests foders in the same folder as the code. This is to help keep the test code linked to the code its self. If you change the code, change the tests. :) TODO: Skip tests that require specific hardware.","title":"Tests"},{"location":"tests/#tests","text":"There is a limited test suite to ensure that the your changes to the code don't break something unintended.","title":"Tests"},{"location":"tests/#run-all-the-tests","text":"Look into the .travis.yml for a more detailed commands used for tests: install section is used to install required packages and setting up environment. script section is actually used to run test suite jobs section may contain another tasks related to other things like tests or deployments. Notice: in .travis.yml env var named TRAVIS_PYTHON_VERSION is populated from python section, such as TRAVIS_PYTHON_VERSION=3.6 . Please refer to the travis documentation for more details.","title":"Run all the tests"},{"location":"tests/#code-organization","text":"The test code is in tests foders in the same folder as the code. This is to help keep the test code linked to the code its self. If you change the code, change the tests. :) TODO: Skip tests that require specific hardware.","title":"Code Organization"},{"location":"guide/build_hardware/","text":"How to Build a Donkey\u00ae Overview Parts Needed Hardware: Step 1: Print Parts Step 2: Clean up parts Step 3: Assemble Top plate and Roll Cage Step 4: Connect Servo Shield to Raspberry Pi Step 5: Attach Raspberry Pi to 3D Printed bottom plate Step 6: Attach Camera Step 7: Put it all together Software Overview These are updated instructions from the 2017 Make Magazine article . The latest version of the software installation instructions are maintained in the software instructions section. Be sure to follow those instructions after you've built your car. Choosing a Car There are 4 fully supported chassis all made under the \"Exceed\" Brand: Exceed Magnet Blue , Red Exceed Desert Monster Blue , Red Exceed Short Course Truck Blue , Red Exceed Blaze Hyper Blue , Yellow These cars are electrically identical but have different tires, mounting and other details. It is worth noting that the Desert Monster, Short Course Truck and Blaze all require adapters which can be easily printed or purchased from the donkey store. These are the standard build cars because they are mostly plug and play, both have a brushed motor which makes training easier, they handle rough driving surfaces well and are inexpensive. In a pinch, the Latrax prerunner also works, with the existing adapters and plastics. LaTrax Prerunner link Here is a video overview of the different cars and how to assemble them. In addition there are 3 more cars supported under the \"Donkey Pro\" name. These are 1/10 scale cars which means that they are bigger, perform a little better and are slightly more expensive. They can be found here: HobbyKing Trooper (not pro version) found here HobbyKing Mission-D found here Tamaya TT01 or Clone - found worldwide but usually has to be built as a kits. The other two cars are ready to be donkified, this one, however is harder to assemble. Here is a video that goes over the different models. The Donkey Pro models are not yet very well documented, just a word of warning. For more detail and other options, follow the link to: supported cars Roll Your Own Car Alternatively If you know RC or need something the standard Donkey does not support, you can roll your own. Here is a quick reference to help you along the way. Roll Your Own Video Overview of Hardware Assembly This video covers how to assemble a standard Donkey Car, it also covers the Sombrero, the Raspberry Pi and the nVidia Jetson Nano. Parts Needed The following instructions are for the Raspbeery Pi, below in Optional Upgrades section, you can find the NVIDIA Jetson Nano instructions. Option 1: Buying through an official Donkey Store There are two official stores: If you are in the US, you can use the Donkey store . The intention of the Donkey Store is to make it easier and less expensive to build the Donkey Car. The Donkey Store is run by the original founders of donkey car and profits are used to fund development of the donkey cars. Also it is worth noting the design of the parts out of the Donkey store is slightly improved over the standard build as it uses better parts that are only available in large quantities or are harder to get. The Donkey Store builds are open source like all others. If you are in Asia, the DIYRobocars community in Hong Kong also sells car kits at Robocar Store . They are long term Donkey community members and use proceeds to support the R&D efforts of this project. It is worth noting they can also sell to Europe and the US but it is likely less cost effective. Part Description Link Approximate Cost Exceed Magnet, Desert Monster, Blaze, or Short Course Truck See links above ~$90 USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh $17 Raspberry Pi 3b+ amazon.com/gp/product/B01CD5VC92 $38 MicroSD Card (many will work, we strongly recommend this one) amazon.com/gp/product/B01HU3Q6F2 $18.99 Donkey Partial Kit KIT $82 to $125 Option 2: Bottoms Up Build If you want to buy the parts yourself, want to customize your donkey or live out to of the US, you may want to choose the bottoms up build. Keep in mind you will have to print the donkey car parts which can be found here Part Description Link Approximate Cost Magnet Car or alternative Blue , Red $92 M2x6 screws (4) Zinc $3.50 * M3x10 screws (8) Black Oxide $7.89 * USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh $17 Raspberry Pi 3b+ amazon.com/gp/product/B01CD5VC92 $38 MicroSD Card (many will work, I like this one because it boots quickly) amazon.com/gp/product/B01HU3Q6F2 $18.99 Wide Angle Raspberry Pi Camera amazon.com/gp/product/B00N1YJKFS $25 Female to Female Jumper Wire amazon.com/gp/product/B010L30SE8 $7 * Servo Driver PCA 9685 amazon.com/gp/product/B014KTSMLA $12 ** 3D Printed roll cage and top plate. Purchase: Donkey Store Files: thingiverse.com/thing:2260575 $50 * If it is hard to find these components there is some wiggle room. Instead of an M2 you can use an M2.2, m2.3 or #4 SAE screw. Instead of an M3 a #6 SAE screw can be used. Machine screws can be used in a pinch. ** This component can be purchased from Ali Express for ~$2-4 if you can wait the 30-60 days for shipping. Optional Upgrades NVIDIA JetsonNano Hardware Options The NVIDIA Jetson Nano is fully supported by the donkey Car. To assemble the Donkey Car you will need a few parts including the Wifi card, Antennas and camera. In addition you will need this Adapter if you want to print it yourself it is on the Thingiverse page for the project. Plug in the Servo driver the same as the Raspberry Pi, just keep in mind that the Jetson pinout is reversed and that the Sombrero is not supported. Finally this is the Donkey Assembled. Part Description Link Approximate Cost Nvidia Jetson Nano Jetson Nano $99 Jetson Nano Adapter Adapter $7 Camera Module Camera $27 WiFi Card Card $18 Antennas Antennas $7 For other options for part, feel free to look at the jetbot documentation here . Sombrero Hat The sombrero hat replaces the Servo driver and the USB battery and can be purchased at the Donkeycar store here and video instructions can be found here . Implementing the Sombrero hat requires a LiPo battery (see below). Documentation is in Github . LiPo Battery and Accessories: LiPo batteries have significantly better energy density and have a better dropoff curve. See below (courtesy of Traxxas). Part Description Link Approximate Cost LiPo Battery hobbyking.com/en_us/turnigy-1800mah-2s-20c-lipo-pack.html or amazon.com/gp/product/B0072AERBE/ $8.94 to $~17 Lipo Charger (takes 1hr to charge the above battery) amazon.com/gp/product/B00XU4ZR06 $13 Lipo Battery Case (to prevent damage if they explode) amazon.com/gp/product/B00T01LLP8 $8 Hardware If you purchased parts from the Donkey Car Store, skip to step 3. Step 1: Print Parts If you do not have a 3D Printer, you can order parts from Donkey Store , Shapeways or 3dHubs . I printed parts in black PLA, with 2mm layer height and no supports. The top roll bar is designed to be printed upside down. Remember that you need to print the adapters unless you have a \"Magnet\" I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down. Step 2: Clean up parts Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below: Step 3: Assemble Top plate and Roll Cage If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video This is a relatively simple assembly step. Just use the 3mm self tapping screws to scew the plate to the roll cage. When attaching the roll cage to the top plate, ensure that the nubs on the top plate face the roll-cage. This will ensure the equipment you mount to the top plate fits easily. Step 4: Connect Servo Shield to Raspberry Pi note: this is not necessary if you have a Sombrero, the Sombrero just plugs into the Pi You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground: Step 5: Attach Raspberry Pi to 3D Printed bottom plate Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro. Step 6: Attach Camera Slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Before using the car, remove the plastic film or lens cover from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this. Step 7: Put it all together Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!! Step 7b: Attach Adapters (Desert Monster only) The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7 Software Congrats! Now to get your get your car moving, see the software instructions section. We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.","title":"How to Build a Donkey&reg;"},{"location":"guide/build_hardware/#how-to-build-a-donkey","text":"Overview Parts Needed Hardware: Step 1: Print Parts Step 2: Clean up parts Step 3: Assemble Top plate and Roll Cage Step 4: Connect Servo Shield to Raspberry Pi Step 5: Attach Raspberry Pi to 3D Printed bottom plate Step 6: Attach Camera Step 7: Put it all together Software","title":"How to Build a Donkey&reg;"},{"location":"guide/build_hardware/#overview","text":"These are updated instructions from the 2017 Make Magazine article . The latest version of the software installation instructions are maintained in the software instructions section. Be sure to follow those instructions after you've built your car.","title":"Overview"},{"location":"guide/build_hardware/#choosing-a-car","text":"There are 4 fully supported chassis all made under the \"Exceed\" Brand: Exceed Magnet Blue , Red Exceed Desert Monster Blue , Red Exceed Short Course Truck Blue , Red Exceed Blaze Hyper Blue , Yellow These cars are electrically identical but have different tires, mounting and other details. It is worth noting that the Desert Monster, Short Course Truck and Blaze all require adapters which can be easily printed or purchased from the donkey store. These are the standard build cars because they are mostly plug and play, both have a brushed motor which makes training easier, they handle rough driving surfaces well and are inexpensive. In a pinch, the Latrax prerunner also works, with the existing adapters and plastics. LaTrax Prerunner link Here is a video overview of the different cars and how to assemble them. In addition there are 3 more cars supported under the \"Donkey Pro\" name. These are 1/10 scale cars which means that they are bigger, perform a little better and are slightly more expensive. They can be found here: HobbyKing Trooper (not pro version) found here HobbyKing Mission-D found here Tamaya TT01 or Clone - found worldwide but usually has to be built as a kits. The other two cars are ready to be donkified, this one, however is harder to assemble. Here is a video that goes over the different models. The Donkey Pro models are not yet very well documented, just a word of warning. For more detail and other options, follow the link to: supported cars","title":"Choosing a Car"},{"location":"guide/build_hardware/#roll-your-own-car","text":"Alternatively If you know RC or need something the standard Donkey does not support, you can roll your own. Here is a quick reference to help you along the way. Roll Your Own","title":"Roll Your Own Car"},{"location":"guide/build_hardware/#video-overview-of-hardware-assembly","text":"This video covers how to assemble a standard Donkey Car, it also covers the Sombrero, the Raspberry Pi and the nVidia Jetson Nano.","title":"Video Overview of Hardware Assembly"},{"location":"guide/build_hardware/#parts-needed","text":"The following instructions are for the Raspbeery Pi, below in Optional Upgrades section, you can find the NVIDIA Jetson Nano instructions.","title":"Parts Needed"},{"location":"guide/build_hardware/#option-1-buying-through-an-official-donkey-store","text":"There are two official stores: If you are in the US, you can use the Donkey store . The intention of the Donkey Store is to make it easier and less expensive to build the Donkey Car. The Donkey Store is run by the original founders of donkey car and profits are used to fund development of the donkey cars. Also it is worth noting the design of the parts out of the Donkey store is slightly improved over the standard build as it uses better parts that are only available in large quantities or are harder to get. The Donkey Store builds are open source like all others. If you are in Asia, the DIYRobocars community in Hong Kong also sells car kits at Robocar Store . They are long term Donkey community members and use proceeds to support the R&D efforts of this project. It is worth noting they can also sell to Europe and the US but it is likely less cost effective. Part Description Link Approximate Cost Exceed Magnet, Desert Monster, Blaze, or Short Course Truck See links above ~$90 USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh $17 Raspberry Pi 3b+ amazon.com/gp/product/B01CD5VC92 $38 MicroSD Card (many will work, we strongly recommend this one) amazon.com/gp/product/B01HU3Q6F2 $18.99 Donkey Partial Kit KIT $82 to $125","title":"Option 1: Buying through an official Donkey Store"},{"location":"guide/build_hardware/#option-2-bottoms-up-build","text":"If you want to buy the parts yourself, want to customize your donkey or live out to of the US, you may want to choose the bottoms up build. Keep in mind you will have to print the donkey car parts which can be found here Part Description Link Approximate Cost Magnet Car or alternative Blue , Red $92 M2x6 screws (4) Zinc $3.50 * M3x10 screws (8) Black Oxide $7.89 * USB Battery with microUSB cable (any battery capable of 2A 5V output is sufficient) Anker 6700 mAh $17 Raspberry Pi 3b+ amazon.com/gp/product/B01CD5VC92 $38 MicroSD Card (many will work, I like this one because it boots quickly) amazon.com/gp/product/B01HU3Q6F2 $18.99 Wide Angle Raspberry Pi Camera amazon.com/gp/product/B00N1YJKFS $25 Female to Female Jumper Wire amazon.com/gp/product/B010L30SE8 $7 * Servo Driver PCA 9685 amazon.com/gp/product/B014KTSMLA $12 ** 3D Printed roll cage and top plate. Purchase: Donkey Store Files: thingiverse.com/thing:2260575 $50 * If it is hard to find these components there is some wiggle room. Instead of an M2 you can use an M2.2, m2.3 or #4 SAE screw. Instead of an M3 a #6 SAE screw can be used. Machine screws can be used in a pinch. ** This component can be purchased from Ali Express for ~$2-4 if you can wait the 30-60 days for shipping.","title":"Option 2: Bottoms Up Build"},{"location":"guide/build_hardware/#optional-upgrades","text":"NVIDIA JetsonNano Hardware Options The NVIDIA Jetson Nano is fully supported by the donkey Car. To assemble the Donkey Car you will need a few parts including the Wifi card, Antennas and camera. In addition you will need this Adapter if you want to print it yourself it is on the Thingiverse page for the project. Plug in the Servo driver the same as the Raspberry Pi, just keep in mind that the Jetson pinout is reversed and that the Sombrero is not supported. Finally this is the Donkey Assembled. Part Description Link Approximate Cost Nvidia Jetson Nano Jetson Nano $99 Jetson Nano Adapter Adapter $7 Camera Module Camera $27 WiFi Card Card $18 Antennas Antennas $7 For other options for part, feel free to look at the jetbot documentation here . Sombrero Hat The sombrero hat replaces the Servo driver and the USB battery and can be purchased at the Donkeycar store here and video instructions can be found here . Implementing the Sombrero hat requires a LiPo battery (see below). Documentation is in Github . LiPo Battery and Accessories: LiPo batteries have significantly better energy density and have a better dropoff curve. See below (courtesy of Traxxas). Part Description Link Approximate Cost LiPo Battery hobbyking.com/en_us/turnigy-1800mah-2s-20c-lipo-pack.html or amazon.com/gp/product/B0072AERBE/ $8.94 to $~17 Lipo Charger (takes 1hr to charge the above battery) amazon.com/gp/product/B00XU4ZR06 $13 Lipo Battery Case (to prevent damage if they explode) amazon.com/gp/product/B00T01LLP8 $8","title":"Optional Upgrades"},{"location":"guide/build_hardware/#hardware","text":"If you purchased parts from the Donkey Car Store, skip to step 3.","title":"Hardware"},{"location":"guide/build_hardware/#step-1-print-parts","text":"If you do not have a 3D Printer, you can order parts from Donkey Store , Shapeways or 3dHubs . I printed parts in black PLA, with 2mm layer height and no supports. The top roll bar is designed to be printed upside down. Remember that you need to print the adapters unless you have a \"Magnet\" I printed parts in black PLA, with .3mm layer height with a .5mm nozzle and no supports. The top roll bar is designed to be printed upside down.","title":"Step 1: Print Parts"},{"location":"guide/build_hardware/#step-2-clean-up-parts","text":"Almost all 3D Printed parts will need clean up. Re-drill holes, and clean up excess plastic. In particular, clean up the slots in the side of the roll bar, as shown in the picture below:","title":"Step 2: Clean up parts"},{"location":"guide/build_hardware/#step-3-assemble-top-plate-and-roll-cage","text":"If you have an Exceed Short Course Truck, Blaze or Desert Monster watch this video This is a relatively simple assembly step. Just use the 3mm self tapping screws to scew the plate to the roll cage. When attaching the roll cage to the top plate, ensure that the nubs on the top plate face the roll-cage. This will ensure the equipment you mount to the top plate fits easily.","title":"Step 3: Assemble Top plate and Roll Cage"},{"location":"guide/build_hardware/#step-4-connect-servo-shield-to-raspberry-pi","text":"note: this is not necessary if you have a Sombrero, the Sombrero just plugs into the Pi You could do this after attaching the Raspberry Pi to the bottom plate, I just think it is easier to see the parts when they are laying on the workbench. Connect the parts as you see below: For reference, below is the Raspberry Pi Pinout for reference. You will notice we connect to 3.3v, the two I2C pins (SDA and SCL) and ground:","title":"Step 4: Connect Servo Shield to Raspberry Pi"},{"location":"guide/build_hardware/#step-5-attach-raspberry-pi-to-3d-printed-bottom-plate","text":"Before you start, now is a good time to insert the already flashed SD card and bench test the electronics. Once that is done, attaching the Raspberry Pi and Servo is as simple as running screws through the board into the screw bosses on the top plate. The M2.5x12mm screws should be the perfect length to go through the board, the plastic and still have room for a washer. The \u201ccap\u201d part of the screw should be facing up and the nut should be on the bottom of the top plate. The ethernet and USB ports should face forward. This is important as it gives you access to the SD card and makes the camera ribbon cable line up properly. Attach the USB battery to the underside of the printed bottom plate using cable ties or velcro.","title":"Step 5: Attach Raspberry Pi to 3D Printed bottom plate"},{"location":"guide/build_hardware/#step-6-attach-camera","text":"Slip the camera into the slot, cable end first. However, be careful not to push on the camera lens and instead press the board. If you need to remove the camera the temptation is to push on the lens, instead push on the connector as is shown in these pictures. Before using the car, remove the plastic film or lens cover from the camera lens. It is easy to put the camera cable in the wrong way so look at these photos and make sure the cable is put in properly. There are loads of tutorials on youtube if you are not used to this.","title":"Step 6: Attach Camera"},{"location":"guide/build_hardware/#step-7-put-it-all-together","text":"Note if you have a Desert Monster Chassis see 7B section below The final steps are straightforward. First attach the roll bar assembly to the car. This is done using the same pins that came with the vehicle. Second run the servo cables up to the car. The throttle cable runs to channel 0 on the servo controller and steering is channel 1. Now you are done with the hardware!!","title":"Step 7: Put it all together"},{"location":"guide/build_hardware/#step-7b-attach-adapters-desert-monster-only","text":"The Desert monster does not have the same set up for holding the body on the car and needs two adapters mentioned above. To attach the adapters you must first remove the existing adapter from the chassis and screw on the custom adapter with the same screws as is shown in this photo: Once this is done, go back to step 7","title":"Step 7b: Attach Adapters (Desert Monster only)"},{"location":"guide/build_hardware/#software","text":"Congrats! Now to get your get your car moving, see the software instructions section. We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.","title":"Software"},{"location":"guide/calibrate/","text":"Calibrate your Car The point of calibrating your car is to make it drive consistently. How to adjust your car's settings You will need to ssh into your Pi to do the calibration. All of the car's settings are in the config.py and myconfig.py scripts generated when you ran the donkey createcar --path ~/mycar command. You can edit this file on your car by running: nano ~/mycar/myconfig.py Steering Calibration Make sure your car is off the ground to prevent a runaway situation. Turn on your car. Find the servo cable on your car and see what channel it's plugged into the PCA board. It should be 1 or 0. Run donkey calibrate --channel <your_steering_channel> --bus=1 Enter 360 and you should see the wheels on your car move slightly. If not enter 400 or 300 . Next enter values +/- 10 from your starting value to find the PWM setting that makes your car turn all the way left and all the way right. Remember these values. Enter these values in myconfig.py script as STEERING_RIGHT_PWM and STEERING_LEFT_PWM . Throttle Calibration Find the cable coming from your ESC and see what channel it goes into the PCA board. This is your throttle channel. run donkey calibrate --channel <your_throttle_channel> --bus=1 Enter 370 when prompted for a PWM value. You should hear your ESC beep indicating that it's calibrated. Enter 400 and you should see your cars wheels start to go forward. If not, its likely that this is reverse, try entering 330 instead. Keep trying different values until you've found a reasonable max speed and remember this PWM value. Reverse on RC cars is a little tricky because the ESC must receive a reverse pulse, zero pulse, reverse pulse to start to go backwards. To calibrate a reverse PWM setting... Use the same technique as above set the PWM setting to your zero throttle. Enter the reverse value, then the zero throttle value, then the reverse value again. Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value. Now open your myconfig.py script and enter the PWM values for your car into the throttle_controller part: THROTTLE_FORWARD_PWM = PWM value for full throttle forward THROTTLE_STOPPED_PWM = PWM value for zero throttle THROTTLE_REVERSE_PWM = PWM value at full reverse throttle Fine tuning your calibration Note : optional Now that you have your car roughly calibrated you can try driving it to verify that it drives as expected. Here's how to fine tune your car's calibration. Start your car by running python manage.py drive . Go to <your_cars_hostname.local>:8887 in a browser. Press j until the cars steering is all the way right. Press i a couple times to get the car to go forward. Measure the diameter of the turn and record it on a spreadsheet. Repeat this measurement for different steering values for turning each direction. Chart these so you can see if your car turns the same in each direction. Corrections: If your car turns the same amount at an 80% turn and a 100% turn, change the PWM setting for that turn direction to be the PWM value at 80%. If your car is biased to turn one direction, change the PWM values of your turns in the opposite direction of the bias. After you've fine tuned your car the steering chart should look something like this. Next let's get driving!","title":"Calibrate your Car"},{"location":"guide/calibrate/#calibrate-your-car","text":"The point of calibrating your car is to make it drive consistently.","title":"Calibrate your Car"},{"location":"guide/calibrate/#how-to-adjust-your-cars-settings","text":"You will need to ssh into your Pi to do the calibration. All of the car's settings are in the config.py and myconfig.py scripts generated when you ran the donkey createcar --path ~/mycar command. You can edit this file on your car by running: nano ~/mycar/myconfig.py","title":"How to adjust your car's settings"},{"location":"guide/calibrate/#steering-calibration","text":"Make sure your car is off the ground to prevent a runaway situation. Turn on your car. Find the servo cable on your car and see what channel it's plugged into the PCA board. It should be 1 or 0. Run donkey calibrate --channel <your_steering_channel> --bus=1 Enter 360 and you should see the wheels on your car move slightly. If not enter 400 or 300 . Next enter values +/- 10 from your starting value to find the PWM setting that makes your car turn all the way left and all the way right. Remember these values. Enter these values in myconfig.py script as STEERING_RIGHT_PWM and STEERING_LEFT_PWM .","title":"Steering Calibration"},{"location":"guide/calibrate/#throttle-calibration","text":"Find the cable coming from your ESC and see what channel it goes into the PCA board. This is your throttle channel. run donkey calibrate --channel <your_throttle_channel> --bus=1 Enter 370 when prompted for a PWM value. You should hear your ESC beep indicating that it's calibrated. Enter 400 and you should see your cars wheels start to go forward. If not, its likely that this is reverse, try entering 330 instead. Keep trying different values until you've found a reasonable max speed and remember this PWM value. Reverse on RC cars is a little tricky because the ESC must receive a reverse pulse, zero pulse, reverse pulse to start to go backwards. To calibrate a reverse PWM setting... Use the same technique as above set the PWM setting to your zero throttle. Enter the reverse value, then the zero throttle value, then the reverse value again. Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value. Now open your myconfig.py script and enter the PWM values for your car into the throttle_controller part: THROTTLE_FORWARD_PWM = PWM value for full throttle forward THROTTLE_STOPPED_PWM = PWM value for zero throttle THROTTLE_REVERSE_PWM = PWM value at full reverse throttle","title":"Throttle Calibration"},{"location":"guide/calibrate/#fine-tuning-your-calibration","text":"Note : optional Now that you have your car roughly calibrated you can try driving it to verify that it drives as expected. Here's how to fine tune your car's calibration. Start your car by running python manage.py drive . Go to <your_cars_hostname.local>:8887 in a browser. Press j until the cars steering is all the way right. Press i a couple times to get the car to go forward. Measure the diameter of the turn and record it on a spreadsheet. Repeat this measurement for different steering values for turning each direction. Chart these so you can see if your car turns the same in each direction. Corrections: If your car turns the same amount at an 80% turn and a 100% turn, change the PWM setting for that turn direction to be the PWM value at 80%. If your car is biased to turn one direction, change the PWM values of your turns in the opposite direction of the bias. After you've fine tuned your car the steering chart should look something like this.","title":"Fine tuning your calibration"},{"location":"guide/calibrate/#next-lets-get-driving","text":"","title":"Next let's get driving!"},{"location":"guide/create_application/","text":"Create your car application If you are not already, please ssh into your vehicle . Create Donkeycar from Template Create a set of files to control your Donkey with this command: donkey createcar --path ~/mycar See also more information on createcar. Configure Options Look at myconfig.py in your newly created directory, ~/mycar cd ~/mycar nano myconfig.py Each line has a comment mark. The commented text shows the default value. When you want to make an edit to over-write the default, uncomment the line by removing the # and any spaces before the first character of the option. example: # STEERING_LEFT_PWM = 460 becomes: STEERING_LEFT_PWM = 500 when edited. You will adjust these later in the calibrate section. Configure I2C PCA9685 If you are using a PCA9685 card, make sure you can see it on I2C. Jetson Nano : sudo usermod -aG i2c $USER sudo reboot After a reboot, then try: sudo i2cdetect -r -y 1 Raspberry Pi : sudo apt-get install -y i2c-tools sudo i2cdetect -y 1 This should show you a grid of addresses like: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- In this case, the 40 shows up as the address of our PCA9685 board. If this does not show up, then check your wiring to the board. On a pi, ensure I2C is enabled in menu of sudo raspi-config (notice, it suggest reboot). If you have assigned a non-standard address to your board, then adjust the address in the myconfig.py under variable PCA9685_I2C_ADDR . If your board is on another bus, then you can specify that with the PCA9685_I2C_BUSNUM . Jetson Nano : set PCA9685_I2C_BUSNUM = 1 in your myconfig.py . For the pi, this will be auto detected by the Adafruit library. But not on the Jetson Nano. Sombrero Setup Set HAVE_SOMBRERO = True in your myconfig.py if you have a sombrero board. Joystick setup If you plan to use a joystick, take a side track over to here . Camera Setup Raspberry Pi : If you are on a raspberry pi and using the recommended pi camera, then no changes are needed to your myconfg.py . Jetson Nano : When using a Sony IMX219 based camera, and you are using the default car template, then you will want edit your myconfg.py to have: CAMERA_TYPE = \"CSIC\" . For flipping the image vertically set CSIC_CAM_GSTREAMER_FLIP_PARM = 3 - this is helpful if you have to mount the camera in a rotated position. Set IMAGE_W = 224 and also IMAGE_H = 224 . CVCAM is a camera type that has worked for USB cameras when OpenCV is setup. This requires additional setup for OpenCV for Nano or OpenCV for Raspberry Pi . WEBCAM is a camera type that uses the pygame library, also typically for USB cameras. That requires additional setup for pygame . Troubleshooting If you are having troubles with your camera, check out our Discourse FAQ for hardware troubleshooting . Check this forum for more help. Keeping Things Up To Date Make all config changes to myconfig.py and they will be preserved through an update. If you are a long time user, you might be used to editing config.py. You should switch to editing myconfig.py instead. Later on, when changes occur that you would like to get, you can pull latest code, then issue a: cd projects/donkeycar git pull donkey createcar --path ~/mycar --overwrite Your ~/mycar/manage.py, ~/mycar/config.py and other files will change with this operation, but myconfig.py will not be touched. Your data and models dirs will not be touched. Note: If you are updating from Donkey<3.0 to 3.0+ it is very likely you will need to start over with a new virtual environment. We've had a few users hit this snag. Next calibrate your car .","title":"Create your car application"},{"location":"guide/create_application/#create-your-car-application","text":"If you are not already, please ssh into your vehicle .","title":"Create your car application"},{"location":"guide/create_application/#create-donkeycar-from-template","text":"Create a set of files to control your Donkey with this command: donkey createcar --path ~/mycar See also more information on createcar.","title":"Create Donkeycar from Template"},{"location":"guide/create_application/#configure-options","text":"Look at myconfig.py in your newly created directory, ~/mycar cd ~/mycar nano myconfig.py Each line has a comment mark. The commented text shows the default value. When you want to make an edit to over-write the default, uncomment the line by removing the # and any spaces before the first character of the option. example: # STEERING_LEFT_PWM = 460 becomes: STEERING_LEFT_PWM = 500 when edited. You will adjust these later in the calibrate section.","title":"Configure Options"},{"location":"guide/create_application/#configure-i2c-pca9685","text":"If you are using a PCA9685 card, make sure you can see it on I2C. Jetson Nano : sudo usermod -aG i2c $USER sudo reboot After a reboot, then try: sudo i2cdetect -r -y 1 Raspberry Pi : sudo apt-get install -y i2c-tools sudo i2cdetect -y 1 This should show you a grid of addresses like: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- In this case, the 40 shows up as the address of our PCA9685 board. If this does not show up, then check your wiring to the board. On a pi, ensure I2C is enabled in menu of sudo raspi-config (notice, it suggest reboot). If you have assigned a non-standard address to your board, then adjust the address in the myconfig.py under variable PCA9685_I2C_ADDR . If your board is on another bus, then you can specify that with the PCA9685_I2C_BUSNUM . Jetson Nano : set PCA9685_I2C_BUSNUM = 1 in your myconfig.py . For the pi, this will be auto detected by the Adafruit library. But not on the Jetson Nano.","title":"Configure I2C PCA9685"},{"location":"guide/create_application/#sombrero-setup","text":"Set HAVE_SOMBRERO = True in your myconfig.py if you have a sombrero board.","title":"Sombrero Setup"},{"location":"guide/create_application/#joystick-setup","text":"If you plan to use a joystick, take a side track over to here .","title":"Joystick setup"},{"location":"guide/create_application/#camera-setup","text":"Raspberry Pi : If you are on a raspberry pi and using the recommended pi camera, then no changes are needed to your myconfg.py . Jetson Nano : When using a Sony IMX219 based camera, and you are using the default car template, then you will want edit your myconfg.py to have: CAMERA_TYPE = \"CSIC\" . For flipping the image vertically set CSIC_CAM_GSTREAMER_FLIP_PARM = 3 - this is helpful if you have to mount the camera in a rotated position. Set IMAGE_W = 224 and also IMAGE_H = 224 . CVCAM is a camera type that has worked for USB cameras when OpenCV is setup. This requires additional setup for OpenCV for Nano or OpenCV for Raspberry Pi . WEBCAM is a camera type that uses the pygame library, also typically for USB cameras. That requires additional setup for pygame .","title":"Camera Setup"},{"location":"guide/create_application/#troubleshooting","text":"If you are having troubles with your camera, check out our Discourse FAQ for hardware troubleshooting . Check this forum for more help.","title":"Troubleshooting"},{"location":"guide/create_application/#keeping-things-up-to-date","text":"Make all config changes to myconfig.py and they will be preserved through an update. If you are a long time user, you might be used to editing config.py. You should switch to editing myconfig.py instead. Later on, when changes occur that you would like to get, you can pull latest code, then issue a: cd projects/donkeycar git pull donkey createcar --path ~/mycar --overwrite Your ~/mycar/manage.py, ~/mycar/config.py and other files will change with this operation, but myconfig.py will not be touched. Your data and models dirs will not be touched. Note: If you are updating from Donkey<3.0 to 3.0+ it is very likely you will need to start over with a new virtual environment. We've had a few users hit this snag.","title":"Keeping Things Up To Date"},{"location":"guide/create_application/#next-calibrate-your-car","text":"","title":"Next calibrate your car."},{"location":"guide/get_driving/","text":"Drive your car After you've calibrated your car you can start driving it. If you are not already, please ssh into your vehicle . Start your car Put your car in a safe place where the wheels are off the ground . This is the step were the car can take off. Open your car's folder and start your car. cd ~/mycar python manage.py drive This script will start the drive loop in your car which includes a part that is a web server for you to control your car. You can now control your car from a web browser at the URL: <your car's hostname.local>:8887 Driving with Web Controller On your phone you can now press start to set your phones current tilt to be zero throttle and steering. Now tilting your phone forward will increase throttle and tilting it side to side will turn the steering. Features Recording - Press record data to start recording images, steering angels and throttle values. Throttle mode - Option to set the throttle as constant. This is used in races if you have a pilot that will steer but doesn't control throttle. Pilot mode - Choose this if the pilot should control the angle and/or throttle. Max throttle - Select the maximum throttle. Keyboard shortcuts space : stop car and stop recording r : toggle recording i : increase throttle k : decrease throttle j : turn left l : turn right If you don't have a joystick then you can skip to next section - train an autopilot . Driving with Physical Joystick Controller You may find that it helps to use a physical joystick device to control your vehicle. Setup Bluetooth and pair joystick Check the Controllers section to read about setting up the bluetooth connection. Start car cd ~/mycar python manage.py drive --js Optionally, if you want joystick use to be sticky and don't want to add the --js each time, modify your myconfig.py so that USE_JOYSTICK_AS_DEFAULT = True nano myconfig.py Joystick Controls Left analog stick - Left and right to adjust steering Right analog stick - Forward to increase forward throttle Pull back twice on right analog to reverse Whenever the throttle is not zero, driving data will be recorded - as long as you are in User mode! Select button switches modes - \"User, Local Angle, Local(angle and throttle)\" Triangle - Increase max throttle X - Decrease max throttle Circle - Toggle recording (disabled by default. auto record on throttle is enabled by default) dpad up - Increase throttle scale dpad down - Decrease throttle scale dpad left - Increase steering scale dpad right - Decrease steering scale Start - Toggle constant throttle. Sets to max throttle (modified by X and Triangle). Next let's train an autopilot .","title":"Drive your car"},{"location":"guide/get_driving/#drive-your-car","text":"After you've calibrated your car you can start driving it. If you are not already, please ssh into your vehicle .","title":"Drive your car"},{"location":"guide/get_driving/#start-your-car","text":"Put your car in a safe place where the wheels are off the ground . This is the step were the car can take off. Open your car's folder and start your car. cd ~/mycar python manage.py drive This script will start the drive loop in your car which includes a part that is a web server for you to control your car. You can now control your car from a web browser at the URL: <your car's hostname.local>:8887","title":"Start your car"},{"location":"guide/get_driving/#driving-with-web-controller","text":"On your phone you can now press start to set your phones current tilt to be zero throttle and steering. Now tilting your phone forward will increase throttle and tilting it side to side will turn the steering.","title":"Driving with Web Controller"},{"location":"guide/get_driving/#features","text":"Recording - Press record data to start recording images, steering angels and throttle values. Throttle mode - Option to set the throttle as constant. This is used in races if you have a pilot that will steer but doesn't control throttle. Pilot mode - Choose this if the pilot should control the angle and/or throttle. Max throttle - Select the maximum throttle.","title":"Features"},{"location":"guide/get_driving/#keyboard-shortcuts","text":"space : stop car and stop recording r : toggle recording i : increase throttle k : decrease throttle j : turn left l : turn right If you don't have a joystick then you can skip to next section - train an autopilot .","title":"Keyboard shortcuts"},{"location":"guide/get_driving/#driving-with-physical-joystick-controller","text":"You may find that it helps to use a physical joystick device to control your vehicle.","title":"Driving with Physical Joystick Controller"},{"location":"guide/get_driving/#setup-bluetooth-and-pair-joystick","text":"Check the Controllers section to read about setting up the bluetooth connection.","title":"Setup Bluetooth and pair joystick"},{"location":"guide/get_driving/#start-car","text":"cd ~/mycar python manage.py drive --js Optionally, if you want joystick use to be sticky and don't want to add the --js each time, modify your myconfig.py so that USE_JOYSTICK_AS_DEFAULT = True nano myconfig.py","title":"Start car"},{"location":"guide/get_driving/#joystick-controls","text":"Left analog stick - Left and right to adjust steering Right analog stick - Forward to increase forward throttle Pull back twice on right analog to reverse Whenever the throttle is not zero, driving data will be recorded - as long as you are in User mode! Select button switches modes - \"User, Local Angle, Local(angle and throttle)\" Triangle - Increase max throttle X - Decrease max throttle Circle - Toggle recording (disabled by default. auto record on throttle is enabled by default) dpad up - Increase throttle scale dpad down - Decrease throttle scale dpad left - Increase steering scale dpad right - Decrease steering scale Start - Toggle constant throttle. Sets to max throttle (modified by X and Triangle).","title":"Joystick Controls"},{"location":"guide/get_driving/#next-lets-train-an-autopilot","text":"","title":"Next let's train an autopilot."},{"location":"guide/install_software/","text":"Install Software Overview Software: Step 1: Install Software on Host PC Step 2: Install Software on Donkeycar Create Donkeycar Application Overview Donkeycar has components to install on a host PC. This can be a laptop, or desktop machine. The machine doesn't have to be powerful, but it will benefit from faster cpu, more ram, and an NVidia GPU. An SSD hard drive will greatly impact your training times. Donkeycar software components need to be installed on the robot platform of your choice. Raspberry Pi and Jetson Nano have setup docs. But it has been known to work on Jetson TX2, Friendly Arm SBC, or almost any Debian based SBC ( single board computer ). After install, you will create the Donkeycar application from a template. This contains code that is designed for you to customize for your particular case. Don't worry, we will get you started with some useful defaults. Next we will train the Donkeycar to drive on it's own based on your driving style! This uses a supervised learning technique often referred to as behavioral cloning. This is not the only method for getting your Donkeycar to drive itself. But it requires the least amount of hardware and least technical knowledge. Then you can explore other techniques in this Ai mobile laboratory called Donkeycar! Step 1: Install Software on Host PC When controlling your Donkey via behavioral cloning, you will need to setup a host pc to train your machine learning model from the data collected on the robot. Choose a setup that matches your computer OS. Setup Linux Host PC Setup Windows Host PC Setup Mac Host PC Step 2: Install Software On Donkeycar This guide will help you to setup the software to run Donkeycar on your Raspberry Pi or Jetson Nano. Choose a setup that matches your SBC type. (SBC = single board computer) Setup RaspberryPi Setup Jetson Nano [Optional] Use TensorRT on the Jetson Nano Read this for more information. Next: Create Your Donkeycar Application .","title":"Install software"},{"location":"guide/install_software/#install-software","text":"Overview Software: Step 1: Install Software on Host PC Step 2: Install Software on Donkeycar Create Donkeycar Application","title":"Install Software"},{"location":"guide/install_software/#overview","text":"Donkeycar has components to install on a host PC. This can be a laptop, or desktop machine. The machine doesn't have to be powerful, but it will benefit from faster cpu, more ram, and an NVidia GPU. An SSD hard drive will greatly impact your training times. Donkeycar software components need to be installed on the robot platform of your choice. Raspberry Pi and Jetson Nano have setup docs. But it has been known to work on Jetson TX2, Friendly Arm SBC, or almost any Debian based SBC ( single board computer ). After install, you will create the Donkeycar application from a template. This contains code that is designed for you to customize for your particular case. Don't worry, we will get you started with some useful defaults. Next we will train the Donkeycar to drive on it's own based on your driving style! This uses a supervised learning technique often referred to as behavioral cloning. This is not the only method for getting your Donkeycar to drive itself. But it requires the least amount of hardware and least technical knowledge. Then you can explore other techniques in this Ai mobile laboratory called Donkeycar!","title":"Overview"},{"location":"guide/install_software/#step-1-install-software-on-host-pc","text":"When controlling your Donkey via behavioral cloning, you will need to setup a host pc to train your machine learning model from the data collected on the robot. Choose a setup that matches your computer OS. Setup Linux Host PC Setup Windows Host PC Setup Mac Host PC","title":"Step 1: Install Software on Host PC"},{"location":"guide/install_software/#step-2-install-software-on-donkeycar","text":"This guide will help you to setup the software to run Donkeycar on your Raspberry Pi or Jetson Nano. Choose a setup that matches your SBC type. (SBC = single board computer) Setup RaspberryPi Setup Jetson Nano","title":"Step 2: Install Software On Donkeycar"},{"location":"guide/install_software/#optional-use-tensorrt-on-the-jetson-nano","text":"Read this for more information.","title":"[Optional] Use TensorRT on the Jetson Nano"},{"location":"guide/install_software/#next-create-your-donkeycar-application","text":"","title":"Next: Create Your Donkeycar Application."},{"location":"guide/simulator/","text":"Donkey Simulator The Donkey Gym project is a OpenAI gym wrapper around the Self Driving Sandbox donkey simulator. When building the sim from source, checkout the donkey branch of the sdsandbox project. The simulator is built on the the Unity game platform, uses their internal physics and graphics, and connects to a donkey Python process to use our trained model to control the simulated Donkey. My Virtual Donkey There are many ways to use the simulator, depending on your goals. You can use the simulator to get to know and use the standard Donkeycar drive/train/test cycle by treating it as virtual hardware. You will collect data, drive, and train using the same commands as if you were using a real robot. We will walk through that use-case first. Install Download and unzip the simulator for your host pc platform from Donkey Gym Release . Place the simulator where you like. For this example it will be ~/projects/DonkeySimLinux. Your dir will have a different name depending on platform. Complete all the steps to install Donkey on your host pc . Setup DonkeyGym: cd ~/projects git clone https://github.com/tawnkramer/gym-donkeycar conda activate donkey pip install -e gym-donkeycar You may use an existing ~/mycar donkey application, or begin a new one. Here we will start fresh: donkey createcar --path ~/mysim cd ~/mysim Edit your myconfig.py to enable donkey gym simulator wrapper, replace <user-name ad the other parts of the path: DONKEY_GYM = True DONKEY_SIM_PATH = \"/home/<user-name>/projects/DonkeySimLinux/donkey_sim.x86_64\" DONKEY_GYM_ENV_NAME = \"donkey-generated-track-v0\" Note: your path to the executable will vary depending on platform and user. Drive You may use all the normal commands to manage.py at this point. Such as: python manage.py drive This should start the simulator and connect to it automatically. By default you will have a web interface to control the donkey. Navigate to http://localhost:8887/drive to see control page. On Ubuntu Linux only, you may plug in your joystick of choice. If it mounts as /dev/input/js0 then there's a good chance it will work. Modify myconfig.py to indicate your joystick model and use the --js arg to run. python manage.py drive --js As you drive, this will create a tub of records in your data dir as usual. Train You will not need to rsync your data, as it was recorded and resides locally. You can train as usual: python manage.py train --model models/mypilot.h5 Test You can use the model as usual: python manage.py drive --model models/mypilot.h5 Then navigate to web control page. Set Mode and Pilot to Local Pilot(d) . The car should start driving. Sample Driving Data Here's some sample driving data to get you started. Download this and unpack it into your data dir. This should train to a slow but stable driver.","title":"Donkey Simulator"},{"location":"guide/simulator/#donkey-simulator","text":"The Donkey Gym project is a OpenAI gym wrapper around the Self Driving Sandbox donkey simulator. When building the sim from source, checkout the donkey branch of the sdsandbox project. The simulator is built on the the Unity game platform, uses their internal physics and graphics, and connects to a donkey Python process to use our trained model to control the simulated Donkey.","title":"Donkey Simulator"},{"location":"guide/simulator/#my-virtual-donkey","text":"There are many ways to use the simulator, depending on your goals. You can use the simulator to get to know and use the standard Donkeycar drive/train/test cycle by treating it as virtual hardware. You will collect data, drive, and train using the same commands as if you were using a real robot. We will walk through that use-case first.","title":"My Virtual Donkey"},{"location":"guide/simulator/#install","text":"Download and unzip the simulator for your host pc platform from Donkey Gym Release . Place the simulator where you like. For this example it will be ~/projects/DonkeySimLinux. Your dir will have a different name depending on platform. Complete all the steps to install Donkey on your host pc . Setup DonkeyGym: cd ~/projects git clone https://github.com/tawnkramer/gym-donkeycar conda activate donkey pip install -e gym-donkeycar You may use an existing ~/mycar donkey application, or begin a new one. Here we will start fresh: donkey createcar --path ~/mysim cd ~/mysim Edit your myconfig.py to enable donkey gym simulator wrapper, replace <user-name ad the other parts of the path: DONKEY_GYM = True DONKEY_SIM_PATH = \"/home/<user-name>/projects/DonkeySimLinux/donkey_sim.x86_64\" DONKEY_GYM_ENV_NAME = \"donkey-generated-track-v0\" Note: your path to the executable will vary depending on platform and user.","title":"Install"},{"location":"guide/simulator/#drive","text":"You may use all the normal commands to manage.py at this point. Such as: python manage.py drive This should start the simulator and connect to it automatically. By default you will have a web interface to control the donkey. Navigate to http://localhost:8887/drive to see control page. On Ubuntu Linux only, you may plug in your joystick of choice. If it mounts as /dev/input/js0 then there's a good chance it will work. Modify myconfig.py to indicate your joystick model and use the --js arg to run. python manage.py drive --js As you drive, this will create a tub of records in your data dir as usual.","title":"Drive"},{"location":"guide/simulator/#train","text":"You will not need to rsync your data, as it was recorded and resides locally. You can train as usual: python manage.py train --model models/mypilot.h5","title":"Train"},{"location":"guide/simulator/#test","text":"You can use the model as usual: python manage.py drive --model models/mypilot.h5 Then navigate to web control page. Set Mode and Pilot to Local Pilot(d) . The car should start driving.","title":"Test"},{"location":"guide/simulator/#sample-driving-data","text":"Here's some sample driving data to get you started. Download this and unpack it into your data dir. This should train to a slow but stable driver.","title":"Sample Driving Data"},{"location":"guide/train_autopilot/","text":"Train an autopilot with Keras Now that you're able to drive your car reliably you can use Keras to train a neural network to drive like you. Here are the steps. Collect Data Make sure you collect good data. Practice driving around the track a couple times. When you're confident you can drive 10 laps without mistake, restart the python mange.py process to create a new tub session. Press Start Recording if using web controller. The joystick will auto record with any non-zero throttle. If you crash or run off the track press Stop Car immediately to stop recording. If you are using a joystick tap the Triangle button to erase the last 5 seconds of records. After you've collected 10-20 laps of good data (5-20k images) you can stop your car with Ctrl-c in the ssh session for your car. The data you've collected is in the data folder in the most recent tub folder. Transfer data from your car to your computer Since the Raspberry Pi is not very powerful, we need to transfer the data to a PC computer to train. The Jetson nano is more powerful, but still quite slow to train. If desired, skip this transfer step and train on the Nano. In a new terminal session on your host PC use rsync to copy your cars folder from the Raspberry Pi. rsync -rv --show-progress --partial pi@<your_pi_ip_address>:~/mycar/data/ ~/mycar/data/ Train a model In the same terminal you can now run the training script on the latest tub by passing the path to that tub as an argument. You can optionally pass path masks, such as ./data/* or ./data/tub_?_17-08-28 to gather multiple tubs. For example: python ~/mycar/manage.py train --tub <tub folder names comma separated> --model ./models/mypilot.h5 Optionally you can pass no arguments for the tub, and then all tubs will be used in the default data dir. python ~/mycar/manage.py train --model ~/mycar/models/mypilot.h5 You can create different model types with the --type argument during training. You may also choose to change the default model type in myconfig.py DEFAULT_MODEL_TYPE . When specifying a new model type, be sure to provide that type when running the model, or using the model in other tools like plotting or profiling. For more information on the different model types, look here for Keras Parts . Now you can use rsync again to move your pilot back to your car. rsync -rv --show-progress --partial ~/mycar/models/ pi@<your_ip_address>:~/mycar/models/ Now you can start your car again and pass it your model to drive. python manage.py drive --model ~/mycar/models/mypilot.h5 [Optional] Use TensorRT on the Jetson Nano Read this for more information. Training Tips Mode & Pilot : Congratulations on getting it this far. The first thing to note after running the command above, is to look at the options in the Mode & Pilot menu. It can be pretty confusing. So here's what the different options mean: a. User : As you guessed, this is where you are in control of both the steering and throttle control. b. Local Angle : Not too obvious, but this is where the trained model (mypilot from above) controls the steering. The Local refers to the trained model which is locally hosted on the raspberry-pi. c. Local Pilot : This is where the trained model (mypilot) assumes control of both the steering and the throttle. As of now, it's purportedly not very reliable. Be sure to also check out the Max Throttle and Throttle Mode options, and play around with a few settings. Can help with training quite a lot. Build a Simple Track : This isn't very well-documented, but the car should (theoretically) be able to train against any kind of track. To start off with, it might not be necessary to build a two-lane track with a striped center-lane. Try with a single lane with no center-line, or just a single strip that makes a circuit! At the least, you'll be able to do an end-to-end testing and verify that the software pipeline is all properly functional. Of course, as the next-step, you'll want to create a more standard track, and compete at a meetup nearest to you! Get help : Try to get some helping hands from a friend or two. Again, this helps immensely with building the track, because it is harder than it looks to build a two-line track on your own! Also, you can save on resources (and tapes) by using a ribbon instead of tapes. They'll still need a bit of tapes to hold them, but you can reuse them and they can be laid down with a lot less effort (Although the wind, if you're working outside, might make it difficult to lay them down initially). Training Behavior Models How to train a Behavior model Make sure TRAIN_BEHAVIORS = True in myconfig.py when training and when running on the robot. Setup an RGB led on robot to indicate which state is active. Enable in config.py. Verify when running robot that L1 PS3 button changes state led indicator. (that's the left upper shoulder button) By default there are two states. If you like, adjust the number of states in bottom of config.py. Rename or change BEHAVIOR_LIST to an arbitrary number of labels. Make sure same number of rgb colors in BEHAVIOR_LED_COLORS . Make sure to reflect any changes to both PC and Robot. Now for training: Activate any state with L1 shoulder button. Then drive as you wish the car to drive when in that state. Switch states and then transition to the new steady state behavior. For the two lane case. Drive 33% in one lane, 33% in the other, and 33% transitioning between them. It's important to trigger the state transition before changing lanes. Check the records in the tub. Open a .json. In addition to steering and throttle, you should also have some additional state information about your behavior vector and which was was activate on that frame. This is crucial to training correctly. Move data to PC and train as normal, ensuring TRAIN_BEHAVIORS = True in myconfig.py on PC, otherwise extra state information will be ignored. Move trained model back to robot. Now place the robot in the location of the initial state. Start the robot with the given model python manage.py drive --model=models/my_beh.h5 --type=behavior Now press select to switch to desired AI mode. Constant throttle available as well as trained throttle. As it drives, you can now toggle states with L1 and see whether and how much it can replicate your steady state behaviors and transitions. Be sure to include quite a lot of example of transitions from one state to another. At least 50, but more like 100.","title":"Train an autopilot with Keras"},{"location":"guide/train_autopilot/#train-an-autopilot-with-keras","text":"Now that you're able to drive your car reliably you can use Keras to train a neural network to drive like you. Here are the steps.","title":"Train an autopilot with Keras"},{"location":"guide/train_autopilot/#collect-data","text":"Make sure you collect good data. Practice driving around the track a couple times. When you're confident you can drive 10 laps without mistake, restart the python mange.py process to create a new tub session. Press Start Recording if using web controller. The joystick will auto record with any non-zero throttle. If you crash or run off the track press Stop Car immediately to stop recording. If you are using a joystick tap the Triangle button to erase the last 5 seconds of records. After you've collected 10-20 laps of good data (5-20k images) you can stop your car with Ctrl-c in the ssh session for your car. The data you've collected is in the data folder in the most recent tub folder.","title":"Collect Data"},{"location":"guide/train_autopilot/#transfer-data-from-your-car-to-your-computer","text":"Since the Raspberry Pi is not very powerful, we need to transfer the data to a PC computer to train. The Jetson nano is more powerful, but still quite slow to train. If desired, skip this transfer step and train on the Nano. In a new terminal session on your host PC use rsync to copy your cars folder from the Raspberry Pi. rsync -rv --show-progress --partial pi@<your_pi_ip_address>:~/mycar/data/ ~/mycar/data/","title":"Transfer data from your car to your computer"},{"location":"guide/train_autopilot/#train-a-model","text":"In the same terminal you can now run the training script on the latest tub by passing the path to that tub as an argument. You can optionally pass path masks, such as ./data/* or ./data/tub_?_17-08-28 to gather multiple tubs. For example: python ~/mycar/manage.py train --tub <tub folder names comma separated> --model ./models/mypilot.h5 Optionally you can pass no arguments for the tub, and then all tubs will be used in the default data dir. python ~/mycar/manage.py train --model ~/mycar/models/mypilot.h5 You can create different model types with the --type argument during training. You may also choose to change the default model type in myconfig.py DEFAULT_MODEL_TYPE . When specifying a new model type, be sure to provide that type when running the model, or using the model in other tools like plotting or profiling. For more information on the different model types, look here for Keras Parts . Now you can use rsync again to move your pilot back to your car. rsync -rv --show-progress --partial ~/mycar/models/ pi@<your_ip_address>:~/mycar/models/ Now you can start your car again and pass it your model to drive. python manage.py drive --model ~/mycar/models/mypilot.h5","title":"Train a model"},{"location":"guide/train_autopilot/#optional-use-tensorrt-on-the-jetson-nano","text":"Read this for more information.","title":"[Optional] Use TensorRT on the Jetson Nano"},{"location":"guide/train_autopilot/#training-tips","text":"Mode & Pilot : Congratulations on getting it this far. The first thing to note after running the command above, is to look at the options in the Mode & Pilot menu. It can be pretty confusing. So here's what the different options mean: a. User : As you guessed, this is where you are in control of both the steering and throttle control. b. Local Angle : Not too obvious, but this is where the trained model (mypilot from above) controls the steering. The Local refers to the trained model which is locally hosted on the raspberry-pi. c. Local Pilot : This is where the trained model (mypilot) assumes control of both the steering and the throttle. As of now, it's purportedly not very reliable. Be sure to also check out the Max Throttle and Throttle Mode options, and play around with a few settings. Can help with training quite a lot. Build a Simple Track : This isn't very well-documented, but the car should (theoretically) be able to train against any kind of track. To start off with, it might not be necessary to build a two-lane track with a striped center-lane. Try with a single lane with no center-line, or just a single strip that makes a circuit! At the least, you'll be able to do an end-to-end testing and verify that the software pipeline is all properly functional. Of course, as the next-step, you'll want to create a more standard track, and compete at a meetup nearest to you! Get help : Try to get some helping hands from a friend or two. Again, this helps immensely with building the track, because it is harder than it looks to build a two-line track on your own! Also, you can save on resources (and tapes) by using a ribbon instead of tapes. They'll still need a bit of tapes to hold them, but you can reuse them and they can be laid down with a lot less effort (Although the wind, if you're working outside, might make it difficult to lay them down initially).","title":"Training Tips"},{"location":"guide/train_autopilot/#training-behavior-models","text":"","title":"Training Behavior Models"},{"location":"guide/train_autopilot/#how-to-train-a-behavior-model","text":"Make sure TRAIN_BEHAVIORS = True in myconfig.py when training and when running on the robot. Setup an RGB led on robot to indicate which state is active. Enable in config.py. Verify when running robot that L1 PS3 button changes state led indicator. (that's the left upper shoulder button) By default there are two states. If you like, adjust the number of states in bottom of config.py. Rename or change BEHAVIOR_LIST to an arbitrary number of labels. Make sure same number of rgb colors in BEHAVIOR_LED_COLORS . Make sure to reflect any changes to both PC and Robot. Now for training: Activate any state with L1 shoulder button. Then drive as you wish the car to drive when in that state. Switch states and then transition to the new steady state behavior. For the two lane case. Drive 33% in one lane, 33% in the other, and 33% transitioning between them. It's important to trigger the state transition before changing lanes. Check the records in the tub. Open a .json. In addition to steering and throttle, you should also have some additional state information about your behavior vector and which was was activate on that frame. This is crucial to training correctly. Move data to PC and train as normal, ensuring TRAIN_BEHAVIORS = True in myconfig.py on PC, otherwise extra state information will be ignored. Move trained model back to robot. Now place the robot in the location of the initial state. Start the robot with the given model python manage.py drive --model=models/my_beh.h5 --type=behavior Now press select to switch to desired AI mode. Constant throttle available as well as trained throttle. As it drives, you can now toggle states with L1 and see whether and how much it can replicate your steady state behaviors and transitions. Be sure to include quite a lot of example of transitions from one state to another. At least 50, but more like 100.","title":"How to train a Behavior model"},{"location":"guide/host_pc/setup_mac/","text":"Install Donkeycar on Mac Install miniconda Python 3.7 64 bit Install git 64 bit Start Terminal Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/mac.yml conda activate donkey pip install -e .[pc] Tensorflow GPU Currently there is no gpu support for tensorflow on mac . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Terminal, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries Next let's install software on Donkeycar","title":"Setup mac"},{"location":"guide/host_pc/setup_mac/#install-donkeycar-on-mac","text":"Install miniconda Python 3.7 64 bit Install git 64 bit Start Terminal Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/mac.yml conda activate donkey pip install -e .[pc] Tensorflow GPU Currently there is no gpu support for tensorflow on mac . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Terminal, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries","title":"Install Donkeycar on Mac"},{"location":"guide/host_pc/setup_mac/#next-lets-install-software-on-donkeycar","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/host_pc/setup_ubuntu/","text":"Install Donkeycar on Linux Note : tested on Ubuntu 18.04 LTS Open the Terminal application. Install miniconda Python 3.7 64 bit . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash ./Miniconda3-latest-Linux-x86_64.sh Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/ubuntu.yml conda activate donkey pip install -e .[pc] Optional Install Tensorflow GPU You should have an NVidia GPU with the latest drivers. Conda will handle installing the correct cuda and cuddn libraries for the version of tensorflow you are using. conda install tensorflow-gpu==1.13.1 Optional Install Coral edge tpu compiler If you have a Google Coral edge tpu, you may wish to compile models. You will need to install the edgetpu_compiler exectutable. Follow their instructions . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries Next let's install software on Donkeycar","title":"Install Donkeycar on Linux"},{"location":"guide/host_pc/setup_ubuntu/#install-donkeycar-on-linux","text":"Note : tested on Ubuntu 18.04 LTS Open the Terminal application. Install miniconda Python 3.7 64 bit . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash ./Miniconda3-latest-Linux-x86_64.sh Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install/envs/ubuntu.yml conda activate donkey pip install -e .[pc] Optional Install Tensorflow GPU You should have an NVidia GPU with the latest drivers. Conda will handle installing the correct cuda and cuddn libraries for the version of tensorflow you are using. conda install tensorflow-gpu==1.13.1 Optional Install Coral edge tpu compiler If you have a Google Coral edge tpu, you may wish to compile models. You will need to install the edgetpu_compiler exectutable. Follow their instructions . Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries","title":"Install Donkeycar on Linux"},{"location":"guide/host_pc/setup_ubuntu/#next-lets-install-software-on-donkeycar","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/host_pc/setup_windows/","text":"Install Donkeycar on Windows Install miniconda Python 3.7 64 bit . Open the Anaconda prompt window via Start Menu | Anaconda 64bit | Anaconda Prompt type git . If the command is not found, then install git 64 bit Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install\\envs\\windows.yml conda activate donkey pip install -e .[pc] Optionally Install Tensorflow GPU If you have an NVidia card, you should update to the lastest drivers and install Cuda SDK . conda install tensorflow-gpu==1.13.1 Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries Next let's install software on Donkeycar","title":"Setup windows"},{"location":"guide/host_pc/setup_windows/#install-donkeycar-on-windows","text":"Install miniconda Python 3.7 64 bit . Open the Anaconda prompt window via Start Menu | Anaconda 64bit | Anaconda Prompt type git . If the command is not found, then install git 64 bit Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkey from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master If this is not your first install, update Conda and remove old donkey conda update -n base -c defaults conda conda env remove -n donkey Create the Python anaconda environment conda env create -f install\\envs\\windows.yml conda activate donkey pip install -e .[pc] Optionally Install Tensorflow GPU If you have an NVidia card, you should update to the lastest drivers and install Cuda SDK . conda install tensorflow-gpu==1.13.1 Create your local working dir: donkey createcar --path ~/mycar Note: After closing the Anaconda Prompt, when you open it again, you will need to type conda activate donkey to re-enable the mappings to donkey specific Python libraries","title":"Install Donkeycar on Windows"},{"location":"guide/host_pc/setup_windows/#next-lets-install-software-on-donkeycar","text":"","title":"Next let's install software on Donkeycar"},{"location":"guide/robot_sbc/setup_jetson_nano/","text":"Get Your Jetson Nano Working Step 1: Flash Operating System Step 2: Install Dependencies Step 3: Setup Virtual Env Step 4: Install OpenCV Step 5: Install Donkeycar Python Code Then Create your Donkeycar Application Step 1: Flash Operating System Visit the official Nvidia Jetson Nano Getting Started Guide . Work through the Prepare for Setup , Writing Image to the microSD Card , and Setup and First Boot instructions, then return here. Step 2: Install Dependencies ssh into your vehicle. Use the the terminal for Ubuntu or Mac. Putty for windows. sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential python3 python3-dev python3-pip libhdf5-serial-dev hdf5-tools nano ntp Optionally, you can install RPi.GPIO clone for Jetson Nano from here . This is not required for default setup, but can be useful if using LED or other GPIO driven devices. Step 3: Setup Virtual Env pip3 install virtualenv python3 -m virtualenv -p python3 env --system-site-packages echo \"source env/bin/activate\" >> ~/.bashrc source ~/.bashrc Step 4: Install OpenCV To install Open CV on the Jetson Nano, you need to build it from source. Building OpenCV from source is going to take some time, so buckle up. If you get stuck, here is another great resource which will help you compile OpenCV. Note: In some cases Python OpenCV may already be installed in your disc image. If the file exists, you can optionally copy it to your environment rather than build from source. Nvidia has said they will drop support for this, so longer term we will probably be building it. If this works: mkdir ~/mycar cp /usr/lib/python3.6/dist-packages/cv2.cpython-36m-aarch64-linux-gnu.so ~/mycar/ cd ~/mycar python -c \"import cv2\" Then you have a working version and can skip this portion of the guide. However, following the swapfile portion of this guide has made performance more predictable and solves memory thrashing. The first step in building OpenCV is to define swap space on the Jetson Nano. The Jetson Nano has 4GB of RAM. This is not sufficient to build OpenCV from source. Therefore we need to define swap space on the Nano to prevent memory thrashing. # Allocates 4G of additional swap space at /var/swapfile sudo fallocate -l 4G /var/swapfile # Permissions sudo chmod 600 /var/swapfile # Make swap space sudo mkswap /var/swapfile # Turn on swap sudo swapon /var/swapfile # Automount swap space on reboot sudo bash -c 'echo \"/var/swapfile swap swap defaults 0 0\" >> /etc/fstab' # Reboot sudo reboot Now you should have enough swap space to build OpenCV. Let's setup the Jetson Nano with the pre-requisites to build OpenCV. # Update sudo apt-get update sudo apt-get upgrade # Pre-requisites sudo apt-get install build-essential cmake unzip pkg-config sudo apt-get install libjpeg-dev libpng-dev libtiff-dev sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev sudo apt-get install libxvidcore-dev libx264-dev sudo apt-get install libgtk-3-dev sudo apt-get install libatlas-base-dev gfortran sudo apt-get install python3-dev Now you should have all the pre-requisites you need. So, lets go ahead and download the source code for OpenCV. # Create a directory for opencv mkdir -p projects/cv2 cd projects/cv2 # Download sources wget -O opencv.zip https://github.com/opencv/opencv/archive/4.1.0.zip wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.1.0.zip # Unzip unzip opencv.zip unzip opencv_contrib.zip # Rename mv opencv-4.1.0 opencv mv opencv_contrib-4.1.0 opencv_contrib Let's get our virtual environment ( env ) ready for OpenCV. # Install Numpy pip install numpy Now let's setup CMake correctly so it generates the correct OpenCV bindings for our virtual environment. # Create a build directory cd projects/cv2/opencv mkdir build cd build # Setup CMake cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D INSTALL_C_EXAMPLES=OFF \\ -D OPENCV_ENABLE_NONFREE=ON \\ # Contrib path -D OPENCV_EXTRA_MODULES_PATH=~/projects/cv2/opencv_contrib/modules \\ # Your virtual environment's Python executable # You need to specify the result of echo $(which python) -D PYTHON_EXECUTABLE=~/env/bin/python \\ -D BUILD_EXAMPLES=ON ../opencv The cmake command should show a summary of the configuration. Make sure that the Interpreter is set to the Python executable associated to your virtualenv. Note: there are several paths in the CMake setup, make sure they match where you downloaded and saved the OpenCV source. To compile the code from the build folder issue the following command. make -j2 This will take a while. Go grab a coffee, or watch a movie. Once the compilation is complete, you are almost done. Only a few more steps to go. # Install OpenCV sudo make install sudo ldconfig The final step is to correctly link the built OpenCV native library to your virtualenv. The native library should now be installed in a location that looks like /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.cpython-36m-xxx-linux-gnu.so . # Go to the folder where OpenCV's native library is built cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6 # Rename mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so # Go to your virtual environments site-packages folder cd ~/env/lib/python3.6/site-packages/ # Symlink the native library ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so Congratulations ! You are now done compiling OpenCV from source. A quick check to see if you did everything correctly is ls -al You should see something that looks like total 48 drwxr-xr-x 10 user user 4096 Jun 16 13:03 . drwxr-xr-x 5 user user 4096 Jun 16 07:46 .. lrwxrwxrwx 1 user user 60 Jun 16 13:03 cv2.so -> /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so -rw-r--r-- 1 user user 126 Jun 16 07:46 easy_install.py drwxr-xr-x 5 user user 4096 Jun 16 07:47 pip drwxr-xr-x 2 user user 4096 Jun 16 07:47 pip-19.1.1.dist-info drwxr-xr-x 5 user user 4096 Jun 16 07:46 pkg_resources drwxr-xr-x 2 user user 4096 Jun 16 07:46 __pycache__ drwxr-xr-x 6 user user 4096 Jun 16 07:46 setuptools drwxr-xr-x 2 user user 4096 Jun 16 07:46 setuptools-41.0.1.dist-info drwxr-xr-x 4 user user 4096 Jun 16 07:47 wheel drwxr-xr-x 2 user user 4096 Jun 16 07:47 wheel-0.33.4.dist-info To test the OpenCV installation, run python and do the following import cv2 # Should print 4.1.0 print(cv2.__version__) Step 5: Install Donkeycar Python Code Change to a dir you would like to use as the head of your projects. cd ~/projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[nano] pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.13.1+nv19.3 Next, create your Donkeycar application .","title":"Get Your Jetson Nano Working"},{"location":"guide/robot_sbc/setup_jetson_nano/#get-your-jetson-nano-working","text":"Step 1: Flash Operating System Step 2: Install Dependencies Step 3: Setup Virtual Env Step 4: Install OpenCV Step 5: Install Donkeycar Python Code Then Create your Donkeycar Application","title":"Get Your Jetson Nano Working"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-1-flash-operating-system","text":"Visit the official Nvidia Jetson Nano Getting Started Guide . Work through the Prepare for Setup , Writing Image to the microSD Card , and Setup and First Boot instructions, then return here.","title":"Step 1: Flash Operating System"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-2-install-dependencies","text":"ssh into your vehicle. Use the the terminal for Ubuntu or Mac. Putty for windows. sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential python3 python3-dev python3-pip libhdf5-serial-dev hdf5-tools nano ntp Optionally, you can install RPi.GPIO clone for Jetson Nano from here . This is not required for default setup, but can be useful if using LED or other GPIO driven devices.","title":"Step 2: Install Dependencies"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-3-setup-virtual-env","text":"pip3 install virtualenv python3 -m virtualenv -p python3 env --system-site-packages echo \"source env/bin/activate\" >> ~/.bashrc source ~/.bashrc","title":"Step 3: Setup Virtual Env"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-4-install-opencv","text":"To install Open CV on the Jetson Nano, you need to build it from source. Building OpenCV from source is going to take some time, so buckle up. If you get stuck, here is another great resource which will help you compile OpenCV. Note: In some cases Python OpenCV may already be installed in your disc image. If the file exists, you can optionally copy it to your environment rather than build from source. Nvidia has said they will drop support for this, so longer term we will probably be building it. If this works: mkdir ~/mycar cp /usr/lib/python3.6/dist-packages/cv2.cpython-36m-aarch64-linux-gnu.so ~/mycar/ cd ~/mycar python -c \"import cv2\" Then you have a working version and can skip this portion of the guide. However, following the swapfile portion of this guide has made performance more predictable and solves memory thrashing. The first step in building OpenCV is to define swap space on the Jetson Nano. The Jetson Nano has 4GB of RAM. This is not sufficient to build OpenCV from source. Therefore we need to define swap space on the Nano to prevent memory thrashing. # Allocates 4G of additional swap space at /var/swapfile sudo fallocate -l 4G /var/swapfile # Permissions sudo chmod 600 /var/swapfile # Make swap space sudo mkswap /var/swapfile # Turn on swap sudo swapon /var/swapfile # Automount swap space on reboot sudo bash -c 'echo \"/var/swapfile swap swap defaults 0 0\" >> /etc/fstab' # Reboot sudo reboot Now you should have enough swap space to build OpenCV. Let's setup the Jetson Nano with the pre-requisites to build OpenCV. # Update sudo apt-get update sudo apt-get upgrade # Pre-requisites sudo apt-get install build-essential cmake unzip pkg-config sudo apt-get install libjpeg-dev libpng-dev libtiff-dev sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev sudo apt-get install libxvidcore-dev libx264-dev sudo apt-get install libgtk-3-dev sudo apt-get install libatlas-base-dev gfortran sudo apt-get install python3-dev Now you should have all the pre-requisites you need. So, lets go ahead and download the source code for OpenCV. # Create a directory for opencv mkdir -p projects/cv2 cd projects/cv2 # Download sources wget -O opencv.zip https://github.com/opencv/opencv/archive/4.1.0.zip wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.1.0.zip # Unzip unzip opencv.zip unzip opencv_contrib.zip # Rename mv opencv-4.1.0 opencv mv opencv_contrib-4.1.0 opencv_contrib Let's get our virtual environment ( env ) ready for OpenCV. # Install Numpy pip install numpy Now let's setup CMake correctly so it generates the correct OpenCV bindings for our virtual environment. # Create a build directory cd projects/cv2/opencv mkdir build cd build # Setup CMake cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D INSTALL_C_EXAMPLES=OFF \\ -D OPENCV_ENABLE_NONFREE=ON \\ # Contrib path -D OPENCV_EXTRA_MODULES_PATH=~/projects/cv2/opencv_contrib/modules \\ # Your virtual environment's Python executable # You need to specify the result of echo $(which python) -D PYTHON_EXECUTABLE=~/env/bin/python \\ -D BUILD_EXAMPLES=ON ../opencv The cmake command should show a summary of the configuration. Make sure that the Interpreter is set to the Python executable associated to your virtualenv. Note: there are several paths in the CMake setup, make sure they match where you downloaded and saved the OpenCV source. To compile the code from the build folder issue the following command. make -j2 This will take a while. Go grab a coffee, or watch a movie. Once the compilation is complete, you are almost done. Only a few more steps to go. # Install OpenCV sudo make install sudo ldconfig The final step is to correctly link the built OpenCV native library to your virtualenv. The native library should now be installed in a location that looks like /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.cpython-36m-xxx-linux-gnu.so . # Go to the folder where OpenCV's native library is built cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6 # Rename mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so # Go to your virtual environments site-packages folder cd ~/env/lib/python3.6/site-packages/ # Symlink the native library ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so Congratulations ! You are now done compiling OpenCV from source. A quick check to see if you did everything correctly is ls -al You should see something that looks like total 48 drwxr-xr-x 10 user user 4096 Jun 16 13:03 . drwxr-xr-x 5 user user 4096 Jun 16 07:46 .. lrwxrwxrwx 1 user user 60 Jun 16 13:03 cv2.so -> /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so -rw-r--r-- 1 user user 126 Jun 16 07:46 easy_install.py drwxr-xr-x 5 user user 4096 Jun 16 07:47 pip drwxr-xr-x 2 user user 4096 Jun 16 07:47 pip-19.1.1.dist-info drwxr-xr-x 5 user user 4096 Jun 16 07:46 pkg_resources drwxr-xr-x 2 user user 4096 Jun 16 07:46 __pycache__ drwxr-xr-x 6 user user 4096 Jun 16 07:46 setuptools drwxr-xr-x 2 user user 4096 Jun 16 07:46 setuptools-41.0.1.dist-info drwxr-xr-x 4 user user 4096 Jun 16 07:47 wheel drwxr-xr-x 2 user user 4096 Jun 16 07:47 wheel-0.33.4.dist-info To test the OpenCV installation, run python and do the following import cv2 # Should print 4.1.0 print(cv2.__version__)","title":"Step 4: Install OpenCV"},{"location":"guide/robot_sbc/setup_jetson_nano/#step-5-install-donkeycar-python-code","text":"Change to a dir you would like to use as the head of your projects. cd ~/projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[nano] pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.13.1+nv19.3","title":"Step 5: Install Donkeycar Python Code"},{"location":"guide/robot_sbc/setup_jetson_nano/#next-create-your-donkeycar-application","text":"","title":"Next, create your Donkeycar application."},{"location":"guide/robot_sbc/setup_raspberry_pi/","text":"Get Your Raspberry Pi Working Step 1: Flash Operating System Step 2: Setup the WiFi for First Boot Step 3: Setup Pi's Hostname Step 4: Enable SSH on Boot Step 5: Connecting to the Pi Step 6: Update and Upgrade Step 7: Raspi-config Step 8: Install Dependencies Step 9: Install Optional OpenCV Dependencies Step 10: Setup Virtual Env Step 11: Install Donkeycar Python Code Step 12: Install Optional OpenCV Then Create your Donkeycar Application Step 1: Flash Operating System You need to flash a micro SD image with an operating system. Download Raspian Lite(Stretch) (352MB). Follow OS specific guides here . Leave micro SD card in your machine and edit/create some files as below: Step 2: Setup the WiFi for first boot We can create a special file which will be used to login to wifi on first boot. More reading here , but we will walk you through it. On Windows, with your memory card image burned and memory disc still inserted, you should see two drives, which are actually two partitions on the mem disc. One is labeled boot . On Mac and Linux, you should also have access to the boot partition of the mem disc. This is formatted with the common FAT type and is where we will edit some files to help it find and log-on to your wifi on its first boot. Note: If boot is not visible right away, try unplugging and re-inserting the memory card reader. Start a text editor: gedit on Linux. Notepad++ on Windows. TextEdit on a Mac. Paste and edit this contents to match your wifi, adjust as needed: country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"<your network name>\" psk=\"<your password>\" } Replace <your network name> with the ID of your network. Leave the quotes. I've seen problems when the network name contained an apostrophe, like \"Joe's iPhone\". Replace <your password> with your password, leaving it surrounded by quotes. If it bothers you to leave your password unencrypted, you may change the contents later once you've gotten the pi to boot and log-in. Save this file to the root of boot partition with the filename wpa_supplicant.conf . On first boot, this file will be moved to /etc/wpa_supplicant/wpa_supplicant.conf where it may be edited later. If you are using Notepad on Windows, make sure it doesn't have a .txt at the end. Step 3: Setup Pi's Hostname Note: This step only possible on a linux host pc. Otherwise you can set it up later in raspi-config after logging in to your pi. We can also setup the hostname so that your Pi easier to find once on the network. If yours is the only Pi on the network, then you can find it with ping raspberrypi.local once it's booted. If there are many other Pi's on the network, then this will have problems. If you are on a Linux machine, or are able to edit the UUID partition, then you can edit the /etc/hostname and /etc/hosts files now to make finding your pi on the network easier after boot. Edit those to replace raspberrypi with a name of your choosing. Use all lower case, no special characters, no hyphens, yes underscores _ . sudo vi /media/userID/UUID/etc/hostname sudo vi /media/userID/UUID/etc/hosts Step 4: Enable SSH on Boot Put a file named ssh in the root of your boot partition. Now your SD card is ready. Eject it from your computer, put it in the Pi and plug in the Pi. Step 5: Connecting to the Pi If you followed the above instructions to add wifi access, your Pi should now be connected to your wifi network. Now you need to find its IP address so you can connect to it via SSH. The easiest way (on Ubuntu) is to use the findcar donkey command. You can try ping raspberrypi.local . If you've modified the hostname, then you should try: ping <your hostname>.local . This will fail on a windows machine. Windows users will need the full IP address (unless using cygwin). If you are having troubles locating your Pi on the network, you will want to plug in an HDMI monitor and USB keyboard into the Pi. Boot it. Login with: Username: pi Password: raspberry Then try the command: ifconfig wlan0 If this has a valid IPv4 address, 4 groups of numbers separated by dots, then you can try that with your SSH command. If you don't see anything like that, then your wifi config might have a mistake. You can try to fix with sudo nano /etc/wpa_supplicant/wpa_supplicant.conf If you don't have a HDMI monitor and keyboard, you can plug-in the Pi with a CAT5 cable to a router with DHCP. If that router is on the same network as your PC, you can try: ping raspberrypi.local Hopefully, one of those methods worked and you are now ready to SSH into your Pi. On Mac and Linux, you can open Terminal. On Windows you can install Putty , one of the alternatives , or on Windows 10 you may have ssh via the command prompt. If you have a command prompt, you can try: ssh pi@raspberrypi.local or ssh pi@<your pi ip address> or via Putty. Username: pi Password: raspberry Hostname: <your pi IP address> Step 6: Update and Upgrade sudo apt-get update sudo apt-get upgrade Step 7: Raspi-config sudo raspi-config change default password for pi change hostname enable Interfacing Options | I2C enable Interfacing Options | Camera Advanced Options | Exapand Filesystem Choose <Finish> and hit enter. Note: Reboot after changing these settings. Should happen if you say yes. Step 8: Install Dependencies sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp Step 9: Install Optional OpenCV Dependencies If you are going for a minimal install, you can get by without these. But it can be handy to have OpenCV. sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test Step 10: Setup Virtual Env python3 -m virtualenv -p python3 env --system-site-packages echo \"source env/bin/activate\" >> ~/.bashrc source ~/.bashrc Modifying your .bashrc in this way will automatically enable this environment each time you login. To return to the system python you can type deactivate . Step 11: Install Donkeycar Python Code Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[pi] pip install tensorflow==1.13.1 You can validate your tensorflow install with python -c \"import tensorflow\" Warnings like this are normal: /home/pi/env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5 return f(*args, **kwds) /home/pi/env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412 return f(*args, **kwds) Note: If you would like to try tflite support, you will need a newer version of Tensorflow. You can download and install this version: For Pi3 Raspian Stretch: wget https://tawn-train.s3.amazonaws.com/tf/tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl pip install tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl For Raspian Buster Python 3.7: TF 2.0 Not yet available. Check slack for updates. Step 12: Install Optional OpenCV If you've opted to install the OpenCV dependencies earlier, you can install Python OpenCV bindings now with sudo apt install python3-opencv If that failed, you can try pip: pip install opencv-python Then test to see if import succeeds. python -c \"import cv2\" And if no errors, you have OpenCV installed! Next, create your Donkeycar application .","title":"Get Your Raspberry Pi Working"},{"location":"guide/robot_sbc/setup_raspberry_pi/#get-your-raspberry-pi-working","text":"Step 1: Flash Operating System Step 2: Setup the WiFi for First Boot Step 3: Setup Pi's Hostname Step 4: Enable SSH on Boot Step 5: Connecting to the Pi Step 6: Update and Upgrade Step 7: Raspi-config Step 8: Install Dependencies Step 9: Install Optional OpenCV Dependencies Step 10: Setup Virtual Env Step 11: Install Donkeycar Python Code Step 12: Install Optional OpenCV Then Create your Donkeycar Application","title":"Get Your Raspberry Pi Working"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-1-flash-operating-system","text":"You need to flash a micro SD image with an operating system. Download Raspian Lite(Stretch) (352MB). Follow OS specific guides here . Leave micro SD card in your machine and edit/create some files as below:","title":"Step 1: Flash Operating System"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-2-setup-the-wifi-for-first-boot","text":"We can create a special file which will be used to login to wifi on first boot. More reading here , but we will walk you through it. On Windows, with your memory card image burned and memory disc still inserted, you should see two drives, which are actually two partitions on the mem disc. One is labeled boot . On Mac and Linux, you should also have access to the boot partition of the mem disc. This is formatted with the common FAT type and is where we will edit some files to help it find and log-on to your wifi on its first boot. Note: If boot is not visible right away, try unplugging and re-inserting the memory card reader. Start a text editor: gedit on Linux. Notepad++ on Windows. TextEdit on a Mac. Paste and edit this contents to match your wifi, adjust as needed: country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"<your network name>\" psk=\"<your password>\" } Replace <your network name> with the ID of your network. Leave the quotes. I've seen problems when the network name contained an apostrophe, like \"Joe's iPhone\". Replace <your password> with your password, leaving it surrounded by quotes. If it bothers you to leave your password unencrypted, you may change the contents later once you've gotten the pi to boot and log-in. Save this file to the root of boot partition with the filename wpa_supplicant.conf . On first boot, this file will be moved to /etc/wpa_supplicant/wpa_supplicant.conf where it may be edited later. If you are using Notepad on Windows, make sure it doesn't have a .txt at the end.","title":"Step 2: Setup the WiFi for first boot"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-3-setup-pis-hostname","text":"Note: This step only possible on a linux host pc. Otherwise you can set it up later in raspi-config after logging in to your pi. We can also setup the hostname so that your Pi easier to find once on the network. If yours is the only Pi on the network, then you can find it with ping raspberrypi.local once it's booted. If there are many other Pi's on the network, then this will have problems. If you are on a Linux machine, or are able to edit the UUID partition, then you can edit the /etc/hostname and /etc/hosts files now to make finding your pi on the network easier after boot. Edit those to replace raspberrypi with a name of your choosing. Use all lower case, no special characters, no hyphens, yes underscores _ . sudo vi /media/userID/UUID/etc/hostname sudo vi /media/userID/UUID/etc/hosts","title":"Step 3: Setup Pi's Hostname"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-4-enable-ssh-on-boot","text":"Put a file named ssh in the root of your boot partition. Now your SD card is ready. Eject it from your computer, put it in the Pi and plug in the Pi.","title":"Step 4: Enable SSH on Boot"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-5-connecting-to-the-pi","text":"If you followed the above instructions to add wifi access, your Pi should now be connected to your wifi network. Now you need to find its IP address so you can connect to it via SSH. The easiest way (on Ubuntu) is to use the findcar donkey command. You can try ping raspberrypi.local . If you've modified the hostname, then you should try: ping <your hostname>.local . This will fail on a windows machine. Windows users will need the full IP address (unless using cygwin). If you are having troubles locating your Pi on the network, you will want to plug in an HDMI monitor and USB keyboard into the Pi. Boot it. Login with: Username: pi Password: raspberry Then try the command: ifconfig wlan0 If this has a valid IPv4 address, 4 groups of numbers separated by dots, then you can try that with your SSH command. If you don't see anything like that, then your wifi config might have a mistake. You can try to fix with sudo nano /etc/wpa_supplicant/wpa_supplicant.conf If you don't have a HDMI monitor and keyboard, you can plug-in the Pi with a CAT5 cable to a router with DHCP. If that router is on the same network as your PC, you can try: ping raspberrypi.local Hopefully, one of those methods worked and you are now ready to SSH into your Pi. On Mac and Linux, you can open Terminal. On Windows you can install Putty , one of the alternatives , or on Windows 10 you may have ssh via the command prompt. If you have a command prompt, you can try: ssh pi@raspberrypi.local or ssh pi@<your pi ip address> or via Putty. Username: pi Password: raspberry Hostname: <your pi IP address>","title":"Step 5: Connecting to the Pi"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-6-update-and-upgrade","text":"sudo apt-get update sudo apt-get upgrade","title":"Step 6: Update and Upgrade"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-7-raspi-config","text":"sudo raspi-config change default password for pi change hostname enable Interfacing Options | I2C enable Interfacing Options | Camera Advanced Options | Exapand Filesystem Choose <Finish> and hit enter. Note: Reboot after changing these settings. Should happen if you say yes.","title":"Step 7: Raspi-config"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-8-install-dependencies","text":"sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp","title":"Step 8: Install Dependencies"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-9-install-optional-opencv-dependencies","text":"If you are going for a minimal install, you can get by without these. But it can be handy to have OpenCV. sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test","title":"Step 9: Install Optional OpenCV Dependencies"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-10-setup-virtual-env","text":"python3 -m virtualenv -p python3 env --system-site-packages echo \"source env/bin/activate\" >> ~/.bashrc source ~/.bashrc Modifying your .bashrc in this way will automatically enable this environment each time you login. To return to the system python you can type deactivate .","title":"Step 10: Setup Virtual Env"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-11-install-donkeycar-python-code","text":"Change to a dir you would like to use as the head of your projects. mkdir projects cd projects Get the latest donkeycar from Github. git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master pip install -e .[pi] pip install tensorflow==1.13.1 You can validate your tensorflow install with python -c \"import tensorflow\" Warnings like this are normal: /home/pi/env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5 return f(*args, **kwds) /home/pi/env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412 return f(*args, **kwds) Note: If you would like to try tflite support, you will need a newer version of Tensorflow. You can download and install this version: For Pi3 Raspian Stretch: wget https://tawn-train.s3.amazonaws.com/tf/tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl pip install tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl For Raspian Buster Python 3.7: TF 2.0 Not yet available. Check slack for updates.","title":"Step 11: Install Donkeycar Python Code"},{"location":"guide/robot_sbc/setup_raspberry_pi/#step-12-install-optional-opencv","text":"If you've opted to install the OpenCV dependencies earlier, you can install Python OpenCV bindings now with sudo apt install python3-opencv If that failed, you can try pip: pip install opencv-python Then test to see if import succeeds. python -c \"import cv2\" And if no errors, you have OpenCV installed!","title":"Step 12: Install Optional OpenCV"},{"location":"guide/robot_sbc/setup_raspberry_pi/#next-create-your-donkeycar-application","text":"","title":"Next, create your Donkeycar application."},{"location":"guide/robot_sbc/tensorrt_jetson_nano/","text":"A Guide to using TensorRT on the Nvidia Jetson Nano Note This guide assumes that you are using Ubuntu 18.04 . If you are using Windows refer to these instructions on how to setup your computer to use TensorRT. Step 1: Setup TensorRT on Ubuntu Machine Follow the instructions here . Make sure you use the tar file instructions unless you have previously installed CUDA using .deb files. Step 2: Setup TensorRT on your Jetson Nano Setup some environment variables so nvcc is on $PATH . Add the following lines to your ~/.bashrc file. # Add this to your .bashrc file export CUDA_HOME=/usr/local/cuda # Adds the CUDA compiler to the PATH export PATH=$CUDA_HOME/bin:$PATH # Adds the libraries export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH Test the changes to your .bashrc . source ~/.bashrc nvcc --version You should see something like: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2018 NVIDIA Corporation Built on ... Cuda compilation tools, release 10.0, Vxxxxx Switch to your virtualenv and install PyCUDA. # This takes a a while.` pip install pycuda After this you will also need to setup PYTHONPATH such that your dist-packages are included as part of your virtualenv . Add this to your .bashrc . This needs to be done because the python bindings to tensorrt are available in dist-packages and this folder is usually not visible to your virtualenv. To make them visible we add it to PYTHONPATH . export PYTHONPATH=/usr/lib/python3.6/dist-packages:$PYTHONPATH Test this change by switching to your virtualenv and importing tensorrt . > import tensorrt as trt > # This import should succeed Step 3: Train, Freeze and Export your model to TensorRT format ( uff ) After you train the linear model you end up with a file with a .h5 extension. # You end up with a Linear.h5 in the models folder python manage.py train --model=./models/Linear.h5 --tub=./data/tub_1_19-06-29,... # Freeze model using freeze_model.py in donkeycar/scripts # The frozen model is stored as protocol buffers. # This command also exports some metadata about the model which is saved in ./models/Linear.metadata python freeze_model.py --model=./models/Linear.h5 --output=./models/Linear.pb # Convert the frozen model to UFF. The command below creates a file ./models/Linear.uff convert-to-uff ./models/Linear.pb Now copy the converted uff model and the metadata to your Jetson Nano. Step 4 In myconfig.py pick the model type as tensorrt_linear . DEFAULT_MODEL_TYPE = `tensorrt_linear` Finally you can do # After you scp your `uff` model to the Nano python manage.py drive --model=./models/Linear.uff","title":"A Guide to using TensorRT on the Nvidia Jetson Nano"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#a-guide-to-using-tensorrt-on-the-nvidia-jetson-nano","text":"Note This guide assumes that you are using Ubuntu 18.04 . If you are using Windows refer to these instructions on how to setup your computer to use TensorRT.","title":"A Guide to using TensorRT on the Nvidia Jetson Nano"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-1-setup-tensorrt-on-ubuntu-machine","text":"Follow the instructions here . Make sure you use the tar file instructions unless you have previously installed CUDA using .deb files.","title":"Step 1: Setup TensorRT on Ubuntu Machine"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-2-setup-tensorrt-on-your-jetson-nano","text":"Setup some environment variables so nvcc is on $PATH . Add the following lines to your ~/.bashrc file. # Add this to your .bashrc file export CUDA_HOME=/usr/local/cuda # Adds the CUDA compiler to the PATH export PATH=$CUDA_HOME/bin:$PATH # Adds the libraries export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH Test the changes to your .bashrc . source ~/.bashrc nvcc --version You should see something like: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2018 NVIDIA Corporation Built on ... Cuda compilation tools, release 10.0, Vxxxxx Switch to your virtualenv and install PyCUDA. # This takes a a while.` pip install pycuda After this you will also need to setup PYTHONPATH such that your dist-packages are included as part of your virtualenv . Add this to your .bashrc . This needs to be done because the python bindings to tensorrt are available in dist-packages and this folder is usually not visible to your virtualenv. To make them visible we add it to PYTHONPATH . export PYTHONPATH=/usr/lib/python3.6/dist-packages:$PYTHONPATH Test this change by switching to your virtualenv and importing tensorrt . > import tensorrt as trt > # This import should succeed","title":"Step 2: Setup TensorRT on your Jetson Nano"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-3-train-freeze-and-export-your-model-to-tensorrt-format-uff","text":"After you train the linear model you end up with a file with a .h5 extension. # You end up with a Linear.h5 in the models folder python manage.py train --model=./models/Linear.h5 --tub=./data/tub_1_19-06-29,... # Freeze model using freeze_model.py in donkeycar/scripts # The frozen model is stored as protocol buffers. # This command also exports some metadata about the model which is saved in ./models/Linear.metadata python freeze_model.py --model=./models/Linear.h5 --output=./models/Linear.pb # Convert the frozen model to UFF. The command below creates a file ./models/Linear.uff convert-to-uff ./models/Linear.pb Now copy the converted uff model and the metadata to your Jetson Nano.","title":"Step 3: Train, Freeze and Export your model to TensorRT format (uff)"},{"location":"guide/robot_sbc/tensorrt_jetson_nano/#step-4","text":"In myconfig.py pick the model type as tensorrt_linear . DEFAULT_MODEL_TYPE = `tensorrt_linear` Finally you can do # After you scp your `uff` model to the Nano python manage.py drive --model=./models/Linear.uff","title":"Step 4"},{"location":"parts/about/","text":"What is a Part A part Python class that wraps a functional component of a vehicle. These include: Sensors - Cameras, Lidar, Odometers, GPS ... Actuators - Motor Controllers Pilots - Lane Detectors, Behavioral Cloning models, ... Controllers - Web based or Bluetooth. Stores - Tub, or a way to save data. Here is an example how to use the PiCamera part to publish an image in the 'cam/img' channel on every drive loop. V = dk.Vehicle() #initialize the camera part cam = PiCamera() #add the part to the vehicle. V.add(cam, outputs=['cam/img']) V.start() Anatomy of a Part All parts share a common structure so that they can all be run by the vehicles drive loop. Here is an example of a part that will accept a number, multiply it by a random number and return the result. import random class RandPercent: def run(self, x): return x * random.random() Now to add this to a vehicle: V = dk.Vehicle() #initialize the channel value V.mem['const'] = 4 #add the part to read and write to the same channel. V.add(RandPercent, inputs=['const'], outputs=['const']) V.start(max_loops=5) Threaded Parts For a vehicle to perform well the drive loop must execute 10-30 times per second so slow parts should be threaded to avoid holding up the drive loop. A threaded part needs to define the function that runs in the separate thread and the function to call that will return the most recent values quickly. Here's an example how to make the RandPercent part threaded if the run function too a second to complete. import random import time class RandPercent: self.in = 0. self.out = 0. def run(self, x): return x * random.random() time.sleep(1) def update(self): #the funtion run in it's own thread while True: self.out = self.run(self.in) def run_threaded(self, x): self.in = x return self.out part.run : function used to run the part part.run_threaded : drive loop function run if part is threaded. part.update : threaded function part.shutdown","title":"What is a Part"},{"location":"parts/about/#what-is-a-part","text":"A part Python class that wraps a functional component of a vehicle. These include: Sensors - Cameras, Lidar, Odometers, GPS ... Actuators - Motor Controllers Pilots - Lane Detectors, Behavioral Cloning models, ... Controllers - Web based or Bluetooth. Stores - Tub, or a way to save data. Here is an example how to use the PiCamera part to publish an image in the 'cam/img' channel on every drive loop. V = dk.Vehicle() #initialize the camera part cam = PiCamera() #add the part to the vehicle. V.add(cam, outputs=['cam/img']) V.start()","title":"What is a Part"},{"location":"parts/about/#anatomy-of-a-part","text":"All parts share a common structure so that they can all be run by the vehicles drive loop. Here is an example of a part that will accept a number, multiply it by a random number and return the result. import random class RandPercent: def run(self, x): return x * random.random() Now to add this to a vehicle: V = dk.Vehicle() #initialize the channel value V.mem['const'] = 4 #add the part to read and write to the same channel. V.add(RandPercent, inputs=['const'], outputs=['const']) V.start(max_loops=5)","title":"Anatomy of a Part"},{"location":"parts/about/#threaded-parts","text":"For a vehicle to perform well the drive loop must execute 10-30 times per second so slow parts should be threaded to avoid holding up the drive loop. A threaded part needs to define the function that runs in the separate thread and the function to call that will return the most recent values quickly. Here's an example how to make the RandPercent part threaded if the run function too a second to complete. import random import time class RandPercent: self.in = 0. self.out = 0. def run(self, x): return x * random.random() time.sleep(1) def update(self): #the funtion run in it's own thread while True: self.out = self.run(self.in) def run_threaded(self, x): self.in = x return self.out part.run : function used to run the part part.run_threaded : drive loop function run if part is threaded. part.update : threaded function part.shutdown","title":"Threaded Parts"},{"location":"parts/actuators/","text":"Acutators Oh noes, nothing in here! This section needs expansion!","title":"Acutators"},{"location":"parts/actuators/#acutators","text":"Oh noes, nothing in here! This section needs expansion!","title":"Acutators"},{"location":"parts/controllers/","text":"Controller Parts Web Controller The default controller to drive the car with your phone or browser. This has a web live preview of camera. Control options include: A virtual joystick The tilt, when using a mobile device with supported accelerometer A physical joystick using the web adapter. Support varies per browser, OS, and joystick combination. Keyboard input via the 'ikjl' keys. Note: Recently iOS has disabled default Safari access to motion control. Joystick Controller Many people find it easier to control the car using a game controller. There are several parts that provide this option. The default web controller may be replaced with a one line change to use a physical joystick part for input. This uses the OS device /dev/input/js0 by default. In theory, any joystick device that the OS mounts like this can be used. In practice, the behavior will change depending on the model of joystick ( Sony, or knockoff ), or XBox controller and the Bluetooth driver used to support it. The default code has been written and tested with a Sony brand PS3 Sixaxis controller . Other controllers may work, but will require alternative Bluetooth installs, and tweaks to the software for correct axis and buttons. These joysticks are known to work: Logitech Gamepad F710 Sony PS3 Sixaxis OEM (Not compatible with Jetson Nano) Sony PS4 Dualshock OEM WiiU Pro XBox Controller SteelSeries Nimbus (works only on TX2 jetpack 4.2+, may work on the Nano) These can be enabled by finding the CONTROLLER_TYPE in your myconfig.py and setting it to the correct string identifier ( after disabling the comment ). These can be used plugged in with a USB cable. It's been much more convenient to setup Bluetooth for a wireless control. There are controller specific setup details below. Customizing or Adding a New Controller Type Note: If are having troubles getting your controller to work, try this Joystick Wizard . This can help customize your buttons and axis inputs as well. Change to config.py or run with --js python manage.py drive --js Will enable driving with the joystick. This disables the live preview of the camera and the web page features. If you modify config.py to make USE_JOYSTICK_AS_DEFAULT = True , then you do not need to run with the --js . PS3 Controller Bluetooth Setup Follow this guide . You can ignore steps past the 'Accessing the SixAxis from Python' section. I will include steps here in case the link becomes stale. sudo apt-get install bluetooth libbluetooth3 libusb-dev sudo systemctl enable bluetooth.service sudo usermod -G bluetooth -a pi Reboot after changing the user group. Plug in the PS3 with USB cable. Hit center PS logo button. Get and build the command line pairing tool. Run it: wget http://www.pabr.org/sixlinux/sixpair.c gcc -o sixpair sixpair.c -lusb sudo ./sixpair Use bluetoothctl to pair bluetoothctl agent on devices trust <MAC ADDRESS> default-agent quit Unplug USB cable. Hit center PS logo button. To test that the Bluetooth PS3 remote is working, verify that /dev/input/js0 exists: ls /dev/input/js0 Troubleshooting In case the BT connection on the Raspberry Pi does not work, you see might something like this in bluetoothctl : [NEW] Controller 00:11:22:33:44:55 super-donkey [default] [NEW] Device AA:BB:CC:DD:EE:FF PLAYSTATION(R)3 Controller [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes ... [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [bluetooth]# Try updating the Linux kernel and firmware by running: sudo rpi-update And then reboot: sudo reboot Charging PS3 Sixaxis Joystick For some reason, they don't like to charge in a powered USB port that doesn't have an active Bluetooth control and OS driver. This means a phone type USB charger will not work. Try a powered Linux or mac laptop USB port. You should see the lights blink after plugging in and hitting center PS logo. After charging, you will need to plug-in the controller again to the Pi, hit the PS logo, then unplug to pair again. New Battery for PS3 Sixaxis Joystick Sometimes these controllers can be quite old. Here's a link to a new battery . Be careful when taking off the cover. Remove 5 screws. There's a tab on the top half between the hand grips. You'll want to split/open it from the front and try pulling the bottom forward as you do, or you'll break the tab off as I did. PS3 Mouse problems on Linux Sometimes when you plug-in the PS3 joystick it starts taking over your mouse. If you want to prevent that, you can run this: xinput set-prop \"Sony PLAYSTATION(R)3 Controller\" \"Device Enabled\" 0 PS4 Controller The following instructions are based on RetroPie and ds4drv . Install ds4drv Running on your pi over ssh, you can directly install it: sudo /home/pi/env/bin/pip install ds4drv Grant permission to ds4drv sudo wget https://raw.githubusercontent.com/chrippa/ds4drv/master/udev/50-ds4drv.rules -O /etc/udev/rules.d/50-ds4drv.rules sudo udevadm control --reload-rules sudo udevadm trigger Run ds4drv ds4drv --hidraw --led 00ff00 If you see Failed to create input device: \"/dev/uinput\" cannot be opened for writing , reboot and retry. Probably granting permission step doesn't take effect until rebooting. Some controllers don't work with --hidraw . If that's the case try the command without it. --led 00ff00 changes the light bar color, it's optional. Start controller in pairing mode Press and hold Share button, then press and hold PS button until the light bar starts blinking. If it goes green after a few seconds, pairing is successful. Run ds4drv in background on startup once booted sudo nano /etc/rc.local paste: /home/pi/env/bin/ds4drv --led 00ff00 Save and exit. Again, with or without --hidraw , depending on the particular controller you are using. To disconnect, kill the process ds4drv and hold PS for 10 seconds to power off the controller. XBox One Controller bluetooth pairing This code presumes the built-in linux driver for 'Xbox Wireless Controller'; this is pre-installed on Raspbian, so there is no need to install any other drivers. This will generally show up on /dev/input/js0. There is another userland driver called xboxdrv; this code has not been tested with that driver. The XBox One controller requires that the bluetooth disable_ertm parameter be set to true; to do this: edit the file /etc/modprobe.d/xbox_bt.conf (that may create the file; it is commonly not there by default) add the line: options bluetooth disable_ertm=1 reboot so that this takes affect. after reboot you can verify that disable_ertm is set to true entering this command in a terminal: bash cat /sys/module/bluetooth/parameters/disable_ertm the result should print 'Y'. If not, make sure the above steps have been done correctly. Once that is done, you can pair your controller to your Raspberry Pi using the bluetooth tool. Enter the following command into a bash shell prompt: sudo bluetoothctl That will start blue tooth pairing in interactive mode. The remaining commands will be entered in that interactive session. Enter the following commands: agent on default-agent scan on That last command will start the Raspberry Pi scanning for new bluetooth devices. At this point, turn on your XBox One controller using the big round 'X' button on top, then start the pairing mode by pressing the 'sync' button on the front of the controller. Within a few minutes, you should see the controller show up in the output something like this; [NEW] Device B8:27:EB:A4:59:08 XBox One Wireless Controller Write down the MAC address, you will need it for the following steps. Enter this command to pair with your controller: connect YOUR_MAC_ADDRESS where YOUR_MAC_ADDRESS is the MAC address you copied previously. If it does not connect on the first try, try again. It can take a few tries. If your controller connects, but then immediately disconnects, your disable_ertm setting might be wrong (see above). Once your controller is connected, the big round 'X' button on the top of your controller should be solid white. Enter the following commands to finish: trust YOUR_MAC_ADDRESS quit Now that your controller is trusted, it should automatically connect with your Raspberry Pi when they are both turned on. If your controller fails to connect, run the bluetoothctl steps again to reconnect. Discovering / Modifying Button and Axis Mappings for Game Controllers To discover and modify your default button mappings (for your controllers) you can use the Joystick class defined in donkeycar.parts.controller . After setting up Donkey and activating your virtualenv you can do the following. First launch python shell session. from donkeycar.parts.controller import Joystick joystick = Joystick() # uses the connected joystick at /dev/input/js0 joystick.init() # Initialize joystick.show_map() # Will give you a list of axes and buttons detected. # Now you can use the controller and check for the outputs. This will # tell you which buttons and axes are active when you are using the # controller. while True: joystick.poll()","title":"Controller Parts"},{"location":"parts/controllers/#controller-parts","text":"","title":"Controller Parts"},{"location":"parts/controllers/#web-controller","text":"The default controller to drive the car with your phone or browser. This has a web live preview of camera. Control options include: A virtual joystick The tilt, when using a mobile device with supported accelerometer A physical joystick using the web adapter. Support varies per browser, OS, and joystick combination. Keyboard input via the 'ikjl' keys. Note: Recently iOS has disabled default Safari access to motion control.","title":"Web Controller"},{"location":"parts/controllers/#joystick-controller","text":"Many people find it easier to control the car using a game controller. There are several parts that provide this option. The default web controller may be replaced with a one line change to use a physical joystick part for input. This uses the OS device /dev/input/js0 by default. In theory, any joystick device that the OS mounts like this can be used. In practice, the behavior will change depending on the model of joystick ( Sony, or knockoff ), or XBox controller and the Bluetooth driver used to support it. The default code has been written and tested with a Sony brand PS3 Sixaxis controller . Other controllers may work, but will require alternative Bluetooth installs, and tweaks to the software for correct axis and buttons.","title":"Joystick Controller"},{"location":"parts/controllers/#these-joysticks-are-known-to-work","text":"Logitech Gamepad F710 Sony PS3 Sixaxis OEM (Not compatible with Jetson Nano) Sony PS4 Dualshock OEM WiiU Pro XBox Controller SteelSeries Nimbus (works only on TX2 jetpack 4.2+, may work on the Nano) These can be enabled by finding the CONTROLLER_TYPE in your myconfig.py and setting it to the correct string identifier ( after disabling the comment ). These can be used plugged in with a USB cable. It's been much more convenient to setup Bluetooth for a wireless control. There are controller specific setup details below.","title":"These joysticks are known to work:"},{"location":"parts/controllers/#customizing-or-adding-a-new-controller-type","text":"Note: If are having troubles getting your controller to work, try this Joystick Wizard . This can help customize your buttons and axis inputs as well.","title":"Customizing or Adding a New Controller Type"},{"location":"parts/controllers/#change-to-configpy-or-run-with-js","text":"python manage.py drive --js Will enable driving with the joystick. This disables the live preview of the camera and the web page features. If you modify config.py to make USE_JOYSTICK_AS_DEFAULT = True , then you do not need to run with the --js .","title":"Change to config.py or run with --js"},{"location":"parts/controllers/#ps3-controller","text":"","title":"PS3 Controller"},{"location":"parts/controllers/#bluetooth-setup","text":"Follow this guide . You can ignore steps past the 'Accessing the SixAxis from Python' section. I will include steps here in case the link becomes stale. sudo apt-get install bluetooth libbluetooth3 libusb-dev sudo systemctl enable bluetooth.service sudo usermod -G bluetooth -a pi Reboot after changing the user group. Plug in the PS3 with USB cable. Hit center PS logo button. Get and build the command line pairing tool. Run it: wget http://www.pabr.org/sixlinux/sixpair.c gcc -o sixpair sixpair.c -lusb sudo ./sixpair Use bluetoothctl to pair bluetoothctl agent on devices trust <MAC ADDRESS> default-agent quit Unplug USB cable. Hit center PS logo button. To test that the Bluetooth PS3 remote is working, verify that /dev/input/js0 exists: ls /dev/input/js0","title":"Bluetooth Setup"},{"location":"parts/controllers/#troubleshooting","text":"In case the BT connection on the Raspberry Pi does not work, you see might something like this in bluetoothctl : [NEW] Controller 00:11:22:33:44:55 super-donkey [default] [NEW] Device AA:BB:CC:DD:EE:FF PLAYSTATION(R)3 Controller [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes ... [CHG] Device AA:BB:CC:DD:EE:FF Connected: yes [CHG] Device AA:BB:CC:DD:EE:FF Connected: no [bluetooth]# Try updating the Linux kernel and firmware by running: sudo rpi-update And then reboot: sudo reboot","title":"Troubleshooting"},{"location":"parts/controllers/#charging-ps3-sixaxis-joystick","text":"For some reason, they don't like to charge in a powered USB port that doesn't have an active Bluetooth control and OS driver. This means a phone type USB charger will not work. Try a powered Linux or mac laptop USB port. You should see the lights blink after plugging in and hitting center PS logo. After charging, you will need to plug-in the controller again to the Pi, hit the PS logo, then unplug to pair again.","title":"Charging PS3 Sixaxis Joystick"},{"location":"parts/controllers/#new-battery-for-ps3-sixaxis-joystick","text":"Sometimes these controllers can be quite old. Here's a link to a new battery . Be careful when taking off the cover. Remove 5 screws. There's a tab on the top half between the hand grips. You'll want to split/open it from the front and try pulling the bottom forward as you do, or you'll break the tab off as I did.","title":"New Battery for PS3 Sixaxis Joystick"},{"location":"parts/controllers/#ps3-mouse-problems-on-linux","text":"Sometimes when you plug-in the PS3 joystick it starts taking over your mouse. If you want to prevent that, you can run this: xinput set-prop \"Sony PLAYSTATION(R)3 Controller\" \"Device Enabled\" 0","title":"PS3 Mouse problems on Linux"},{"location":"parts/controllers/#ps4-controller","text":"The following instructions are based on RetroPie and ds4drv .","title":"PS4 Controller"},{"location":"parts/controllers/#install-ds4drv","text":"Running on your pi over ssh, you can directly install it: sudo /home/pi/env/bin/pip install ds4drv","title":"Install ds4drv"},{"location":"parts/controllers/#grant-permission-to-ds4drv","text":"sudo wget https://raw.githubusercontent.com/chrippa/ds4drv/master/udev/50-ds4drv.rules -O /etc/udev/rules.d/50-ds4drv.rules sudo udevadm control --reload-rules sudo udevadm trigger","title":"Grant permission to ds4drv"},{"location":"parts/controllers/#run-ds4drv","text":"ds4drv --hidraw --led 00ff00 If you see Failed to create input device: \"/dev/uinput\" cannot be opened for writing , reboot and retry. Probably granting permission step doesn't take effect until rebooting. Some controllers don't work with --hidraw . If that's the case try the command without it. --led 00ff00 changes the light bar color, it's optional.","title":"Run ds4drv"},{"location":"parts/controllers/#start-controller-in-pairing-mode","text":"Press and hold Share button, then press and hold PS button until the light bar starts blinking. If it goes green after a few seconds, pairing is successful.","title":"Start controller in pairing mode"},{"location":"parts/controllers/#run-ds4drv-in-background-on-startup-once-booted","text":"sudo nano /etc/rc.local paste: /home/pi/env/bin/ds4drv --led 00ff00 Save and exit. Again, with or without --hidraw , depending on the particular controller you are using. To disconnect, kill the process ds4drv and hold PS for 10 seconds to power off the controller.","title":"Run ds4drv in background on startup once booted"},{"location":"parts/controllers/#xbox-one-controller","text":"","title":"XBox One Controller"},{"location":"parts/controllers/#bluetooth-pairing","text":"This code presumes the built-in linux driver for 'Xbox Wireless Controller'; this is pre-installed on Raspbian, so there is no need to install any other drivers. This will generally show up on /dev/input/js0. There is another userland driver called xboxdrv; this code has not been tested with that driver. The XBox One controller requires that the bluetooth disable_ertm parameter be set to true; to do this: edit the file /etc/modprobe.d/xbox_bt.conf (that may create the file; it is commonly not there by default) add the line: options bluetooth disable_ertm=1 reboot so that this takes affect. after reboot you can verify that disable_ertm is set to true entering this command in a terminal: bash cat /sys/module/bluetooth/parameters/disable_ertm the result should print 'Y'. If not, make sure the above steps have been done correctly. Once that is done, you can pair your controller to your Raspberry Pi using the bluetooth tool. Enter the following command into a bash shell prompt: sudo bluetoothctl That will start blue tooth pairing in interactive mode. The remaining commands will be entered in that interactive session. Enter the following commands: agent on default-agent scan on That last command will start the Raspberry Pi scanning for new bluetooth devices. At this point, turn on your XBox One controller using the big round 'X' button on top, then start the pairing mode by pressing the 'sync' button on the front of the controller. Within a few minutes, you should see the controller show up in the output something like this; [NEW] Device B8:27:EB:A4:59:08 XBox One Wireless Controller Write down the MAC address, you will need it for the following steps. Enter this command to pair with your controller: connect YOUR_MAC_ADDRESS where YOUR_MAC_ADDRESS is the MAC address you copied previously. If it does not connect on the first try, try again. It can take a few tries. If your controller connects, but then immediately disconnects, your disable_ertm setting might be wrong (see above). Once your controller is connected, the big round 'X' button on the top of your controller should be solid white. Enter the following commands to finish: trust YOUR_MAC_ADDRESS quit Now that your controller is trusted, it should automatically connect with your Raspberry Pi when they are both turned on. If your controller fails to connect, run the bluetoothctl steps again to reconnect.","title":"bluetooth pairing"},{"location":"parts/controllers/#discovering-modifying-button-and-axis-mappings-for-game-controllers","text":"To discover and modify your default button mappings (for your controllers) you can use the Joystick class defined in donkeycar.parts.controller . After setting up Donkey and activating your virtualenv you can do the following. First launch python shell session. from donkeycar.parts.controller import Joystick joystick = Joystick() # uses the connected joystick at /dev/input/js0 joystick.init() # Initialize joystick.show_map() # Will give you a list of axes and buttons detected. # Now you can use the controller and check for the outputs. This will # tell you which buttons and axes are active when you are using the # controller. while True: joystick.poll()","title":"Discovering / Modifying Button and Axis Mappings for Game Controllers"},{"location":"parts/imu/","text":"IMU IMUs or inertial measurement units are parts that sense the inertial forces on a robot. They vary depending on sensor, but may commonly include linear and rotational accelleration. They may sometimes include magnetometer to give global compasss facing dir. Frequently temperature is available from these as it affects their sensitivity. MPU6050 This is a cheap, small, and moderately precise imu. Commonly available at Amazon . Typically uses the I2C interface and can be chained off the default PWM PCA9685 board. This configuration will also provide power. Outputs acceleration X, Y, Z, Gyroscope X, Y, Z, and temperature. Chip built-in 16bit AD converter, 16bit data output Gyroscopes range: +/- 250 500 1000 2000 degree/sec Acceleration range: \u00b12 \u00b14 \u00b18 \u00b116g Software Setup Install smbus: from package: sudo apt install python3-smbus or from source: sudo apt-get install i2c-tools libi2c-dev python-dev python3-dev git clone https://github.com/pimoroni/py-smbus.git cd py-smbus/library python setup.py build sudo python setup.py install Install pip lib for mpu6050 : pip install mpu6050-raspberrypi","title":"IMU"},{"location":"parts/imu/#imu","text":"IMUs or inertial measurement units are parts that sense the inertial forces on a robot. They vary depending on sensor, but may commonly include linear and rotational accelleration. They may sometimes include magnetometer to give global compasss facing dir. Frequently temperature is available from these as it affects their sensitivity.","title":"IMU"},{"location":"parts/imu/#mpu6050","text":"This is a cheap, small, and moderately precise imu. Commonly available at Amazon . Typically uses the I2C interface and can be chained off the default PWM PCA9685 board. This configuration will also provide power. Outputs acceleration X, Y, Z, Gyroscope X, Y, Z, and temperature. Chip built-in 16bit AD converter, 16bit data output Gyroscopes range: +/- 250 500 1000 2000 degree/sec Acceleration range: \u00b12 \u00b14 \u00b18 \u00b116g","title":"MPU6050"},{"location":"parts/imu/#software-setup","text":"Install smbus: from package: sudo apt install python3-smbus or from source: sudo apt-get install i2c-tools libi2c-dev python-dev python3-dev git clone https://github.com/pimoroni/py-smbus.git cd py-smbus/library python setup.py build sudo python setup.py install Install pip lib for mpu6050 : pip install mpu6050-raspberrypi","title":"Software Setup"},{"location":"parts/keras/","text":"Keras Parts These parts encapsulate models defined using the Keras high level api. They are intended to be used with the Tensorflow backend. The parts are designed to use the trained artificial neural network to reproduce the steering and throttle given the image the camera sees. They are created by using the train command . Keras Categorical This model type is created with the --type=catagorical . The KerasCategorical pilot breaks the steering and throttle decisions into discreet angles and then uses categorical cross entropy to train the network to activate a single neuron for each steering and throttle choice. This can be interesting because we get the confidence value as a distribution over all choices. This uses the dk.utils.linear_bin and dk.utils.linear_unbin to transform continuous real numbers into a range of discreet values for training and runtime. The input and output are therefore bounded and must be chosen wisely to match the data. The default ranges work for the default setup. But cars which go faster may want to enable a higher throttle range. And cars with larger steering throw may want more bins. This model was the original model, with some modifications, when Donkey was first created. Pros It has some benefits of showing the confidense as a distribution via the makemovie command. It has been very robust. In some cases this model has learned thottle control better than other models. Performs well in a limited compute environment like the Pi3. Cons Suffers from some arbitrary limitations of the chosen limits for number of categories, and thottle upper limit. Model Summary Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers, 16, and 20 w categorical output Keras Linear This model type is created with the --type=linear . The KerasLinear pilot uses one neuron to output a continous value via the Keras Dense layer with linear activation. One each for steering and throttle. The output is not bounded. Pros Steers smoothly. It has been very robust. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Cons May sometimes fail to learn throttle well. Model Summary Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Keras IMU This model type is created with the --type=imu . The KerasIMU pilot is very similar to the KerasLinear model, except that it takes intertial measurment data in addition to images when learning to drive. This gives our stateless model some additional information about the motion of the vehicle. This can be a good starting point example of ingesting more data into your models. Pros Steers very smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Gives additional state to the model, which might help it come to a stop at a stop sign. Cons Driving quality will suffer if noisy imu is used. Model Summary Input: Image, vector of linear and angular acceleration Network: 5 Convolution layers followed by two dense layers before output, Vector data is followed by 3 dense layers then concatenating before 2 dense control layers and after conv2d layers. Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Keras Latent This model type is created with the --type=latent . The KerasLatent pilot tries to force the model to learn a latent vector in addition to driving. This latent vector is a bottleneck in a CNN that then tries to reproduce the given input image and produce driving commands. These dual tasks could produce a model that learns to distill the driving scene and perhaps better abstract to a new track. Pros Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Image output a measure of what the model has deemed important in the scene. Cons Needs more testing to prove theory. Model Summary Input: Image Network: 5 Convolution layers bottleneck to a 10x1x1 vector, followed by 6Conv2dTranspose layers before outputing to a image and 3 dense layers and driving controls. Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Outputs an image. Keras RNN This model type is created with the --type=rnn . The KerasRNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py. Pros Steers very smoothly. Can train to a lower loss Cons Performs worse in a limited compute environment like the Pi3. Takes longer to train. Model Summary Input: Image Network: 4 time distributed Convolution layers, followed by 2 LSTM layers, 3 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle. Keras 3D This model type is created with the --type=3d . The Keras3D_CNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py. Instead of 2d convolutions like most other models, this uses a 3D convolution across layers. Pros Steers very smoothly. Can train to a lower loss. Cons Performs worse in a limited compute environment like the Pi3. Takes longer to train. Model Summary Input: Image Network: 4 3D Convolution layers each followed by max pooling, followed by 2 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle. Keras Behavior This model type is created with the --type=behavior . The KerasBehavioral pilot takes an image and a vector as input. The vector is one hot activated vector of commands. This vector might be of length two and have two states, one for left lane driving and one for right lane driving. Then during training one element of the vector is activated while the desired behavior is demonstrated. This vector is defined in myconfig.py BEHAVIOR_LIST . BEHAVIOR_LED_COLORS must match the same length and can be useful when showing the current state. TRAIN_BEHAVIORS must be set to True. Pros Can create a model which can perform multiple tasks Cons Takes more effort to train. Model Summary Input: Image, Behavior vector Network: 5 Convolution layers, followed by 2 dense layers, and driving controls. Output: Categorical steering, throttle output similar to Categorical keras model. Keras Localizer This model type is not created without some code modification. The KerasLocalizer pilot is very similar to the Keras Linear model, except that it learns to output it's location as a category. This category is arbitrary, but has only been tested as a 0-9 range segment of the track. This requires that the driving data is marked up with a category label for location. This could supply some higher level logic with track location, for driving stategy, lap counting, or other. Pros Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Location to supply some higher level logic. Cons May sometimes fail to learn throttle well. Model Summary Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle. One categorical output for location.","title":"Keras Parts"},{"location":"parts/keras/#keras-parts","text":"These parts encapsulate models defined using the Keras high level api. They are intended to be used with the Tensorflow backend. The parts are designed to use the trained artificial neural network to reproduce the steering and throttle given the image the camera sees. They are created by using the train command .","title":"Keras Parts"},{"location":"parts/keras/#keras-categorical","text":"This model type is created with the --type=catagorical . The KerasCategorical pilot breaks the steering and throttle decisions into discreet angles and then uses categorical cross entropy to train the network to activate a single neuron for each steering and throttle choice. This can be interesting because we get the confidence value as a distribution over all choices. This uses the dk.utils.linear_bin and dk.utils.linear_unbin to transform continuous real numbers into a range of discreet values for training and runtime. The input and output are therefore bounded and must be chosen wisely to match the data. The default ranges work for the default setup. But cars which go faster may want to enable a higher throttle range. And cars with larger steering throw may want more bins. This model was the original model, with some modifications, when Donkey was first created.","title":"Keras Categorical"},{"location":"parts/keras/#pros","text":"It has some benefits of showing the confidense as a distribution via the makemovie command. It has been very robust. In some cases this model has learned thottle control better than other models. Performs well in a limited compute environment like the Pi3.","title":"Pros"},{"location":"parts/keras/#cons","text":"Suffers from some arbitrary limitations of the chosen limits for number of categories, and thottle upper limit.","title":"Cons"},{"location":"parts/keras/#model-summary","text":"Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers, 16, and 20 w categorical output","title":"Model Summary"},{"location":"parts/keras/#keras-linear","text":"This model type is created with the --type=linear . The KerasLinear pilot uses one neuron to output a continous value via the Keras Dense layer with linear activation. One each for steering and throttle. The output is not bounded.","title":"Keras Linear"},{"location":"parts/keras/#pros_1","text":"Steers smoothly. It has been very robust. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle.","title":"Pros"},{"location":"parts/keras/#cons_1","text":"May sometimes fail to learn throttle well.","title":"Cons"},{"location":"parts/keras/#model-summary_1","text":"Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-imu","text":"This model type is created with the --type=imu . The KerasIMU pilot is very similar to the KerasLinear model, except that it takes intertial measurment data in addition to images when learning to drive. This gives our stateless model some additional information about the motion of the vehicle. This can be a good starting point example of ingesting more data into your models.","title":"Keras IMU"},{"location":"parts/keras/#pros_2","text":"Steers very smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Gives additional state to the model, which might help it come to a stop at a stop sign.","title":"Pros"},{"location":"parts/keras/#cons_2","text":"Driving quality will suffer if noisy imu is used.","title":"Cons"},{"location":"parts/keras/#model-summary_2","text":"Input: Image, vector of linear and angular acceleration Network: 5 Convolution layers followed by two dense layers before output, Vector data is followed by 3 dense layers then concatenating before 2 dense control layers and after conv2d layers. Output: Two dense layers with one scalar output each with linear activation for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-latent","text":"This model type is created with the --type=latent . The KerasLatent pilot tries to force the model to learn a latent vector in addition to driving. This latent vector is a bottleneck in a CNN that then tries to reproduce the given input image and produce driving commands. These dual tasks could produce a model that learns to distill the driving scene and perhaps better abstract to a new track.","title":"Keras Latent"},{"location":"parts/keras/#pros_3","text":"Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Image output a measure of what the model has deemed important in the scene.","title":"Pros"},{"location":"parts/keras/#cons_3","text":"Needs more testing to prove theory.","title":"Cons"},{"location":"parts/keras/#model-summary_3","text":"Input: Image Network: 5 Convolution layers bottleneck to a 10x1x1 vector, followed by 6Conv2dTranspose layers before outputing to a image and 3 dense layers and driving controls. Output: Two dense layers with one scalar output each with linear activation for steering and throttle. Outputs an image.","title":"Model Summary"},{"location":"parts/keras/#keras-rnn","text":"This model type is created with the --type=rnn . The KerasRNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py.","title":"Keras RNN"},{"location":"parts/keras/#pros_4","text":"Steers very smoothly. Can train to a lower loss","title":"Pros"},{"location":"parts/keras/#cons_4","text":"Performs worse in a limited compute environment like the Pi3. Takes longer to train.","title":"Cons"},{"location":"parts/keras/#model-summary_4","text":"Input: Image Network: 4 time distributed Convolution layers, followed by 2 LSTM layers, 3 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-3d","text":"This model type is created with the --type=3d . The Keras3D_CNN pilot uses a sequence of images to control driving rather than just a single frame. The number of images used is controlled by the SEQUENCE_LENGTH value in myconfig.py. Instead of 2d convolutions like most other models, this uses a 3D convolution across layers.","title":"Keras 3D"},{"location":"parts/keras/#pros_5","text":"Steers very smoothly. Can train to a lower loss.","title":"Pros"},{"location":"parts/keras/#cons_5","text":"Performs worse in a limited compute environment like the Pi3. Takes longer to train.","title":"Cons"},{"location":"parts/keras/#model-summary_5","text":"Input: Image Network: 4 3D Convolution layers each followed by max pooling, followed by 2 dense layers, and driving controls. Output: One dense layer with two scalar outputs for steering and throttle.","title":"Model Summary"},{"location":"parts/keras/#keras-behavior","text":"This model type is created with the --type=behavior . The KerasBehavioral pilot takes an image and a vector as input. The vector is one hot activated vector of commands. This vector might be of length two and have two states, one for left lane driving and one for right lane driving. Then during training one element of the vector is activated while the desired behavior is demonstrated. This vector is defined in myconfig.py BEHAVIOR_LIST . BEHAVIOR_LED_COLORS must match the same length and can be useful when showing the current state. TRAIN_BEHAVIORS must be set to True.","title":"Keras Behavior"},{"location":"parts/keras/#pros_6","text":"Can create a model which can perform multiple tasks","title":"Pros"},{"location":"parts/keras/#cons_6","text":"Takes more effort to train.","title":"Cons"},{"location":"parts/keras/#model-summary_6","text":"Input: Image, Behavior vector Network: 5 Convolution layers, followed by 2 dense layers, and driving controls. Output: Categorical steering, throttle output similar to Categorical keras model.","title":"Model Summary"},{"location":"parts/keras/#keras-localizer","text":"This model type is not created without some code modification. The KerasLocalizer pilot is very similar to the Keras Linear model, except that it learns to output it's location as a category. This category is arbitrary, but has only been tested as a 0-9 range segment of the track. This requires that the driving data is marked up with a category label for location. This could supply some higher level logic with track location, for driving stategy, lap counting, or other.","title":"Keras Localizer"},{"location":"parts/keras/#pros_7","text":"Steers smoothly. Performs well in a limited compute environment like the Pi3. No arbitrary limits to steering or throttle. Location to supply some higher level logic.","title":"Pros"},{"location":"parts/keras/#cons_7","text":"May sometimes fail to learn throttle well.","title":"Cons"},{"location":"parts/keras/#model-summary_7","text":"Input: Image Network: 5 Convolution layers followed by two dense layers before output Output: Two dense layers with one scalar output each with linear activation for steering and throttle. One categorical output for location.","title":"Model Summary"},{"location":"parts/stores/","text":"Stores Stores are parts that record and replay vehicle data produced by other parts. Tub This is the standard donkey data store and it is modeled after the ROSBAG. TODO: The structure of the Tub part is not ideal and should be changed. types should not need to be specified and could be inspected and saved on the first loop. Example creation import donkey as dk T = dk.parts.Tub(path, inputs, types) Accepted Types float - saved as record int - saved as record","title":"Stores"},{"location":"parts/stores/#stores","text":"Stores are parts that record and replay vehicle data produced by other parts.","title":"Stores"},{"location":"parts/stores/#tub","text":"This is the standard donkey data store and it is modeled after the ROSBAG. TODO: The structure of the Tub part is not ideal and should be changed. types should not need to be specified and could be inspected and saved on the first loop. Example creation import donkey as dk T = dk.parts.Tub(path, inputs, types)","title":"Tub"},{"location":"parts/stores/#accepted-types","text":"float - saved as record int - saved as record","title":"Accepted Types"},{"location":"utility/donkey/","text":"Donkey Command-line Utilities The donkey command is created when you install the donkeycar Python package. This is a Python script that adds some important functionality. The operations here are vehicle independent, and should work on any hardware configuration. Create Car This command creates a new dir which will contain the files needed to run and train your robot. Usage: donkey createcar --path <dir> [--overwrite] [--template <donkey2>] This command may be run from any dir Run on the host computer or the robot It uses the --path as the destination dir to create. If .py files exist there, it will not overwrite them, unless the optional --overwrite is used. The optional --template will specify the template file to start from. For a list of templates, see the donkeycar/templates dir. This source template will be copied over the manage.py for the user. Find Car This command attempts to locate your car on the local network using nmap. Usage: donkey findcar Run on the host computer Prints the host computer IP address and the car IP address if found Requires the nmap utility: sudo apt install nmap Calibrate Car This command allows you to manually enter values to interactively set the PWM values and experiment with how your robot responds. See also more information. Usage: donkey calibrate --channel <0-15 channel id> Run on the host computer Opens the PWM channel specified by --channel Type integer values to specify PWM values and hit enter Hit Ctrl + C to exit Clean data in Tub Opens a web server to delete bad data from a tub. Usage: donkey tubclean <folder containing tubs> Run on pi or host computer. Opens the web server to delete bad data. Hit Ctrl + C to exit Make Movie from Tub This command allows you to create a movie file from the images in a Tub. Usage: donkey makemovie --tub=<tub_path> [--out=<tub_movie.mp4>] [--config=<config.py>] [--model=<model path>] [--model_type=(linear|categorical|rnn|imu|behavior|3d)] [--start=0] [--end=-1] [--scale=2] [--salient] Run on the host computer or the robot Uses the image records from --tub dir path given Creates a movie given by --out . Codec is inferred from file extension. Default: tub_movie.mp4 Optional argument to specify a different config.py other than default: config.py Optional model argument will load the keras model and display prediction as lines on the movie model_type may optionally give a hint about what model type we are loading. Categorical is default. optional --salient will overlay a visualization of which pixels excited the NN the most optional --start and/or --end can specify a range of frame numbers to use. scale will cause ouput image to be scaled by this amount Check Tub This command allows you to see how many records are contained in any/all tubs. It will also open each record and ensure that the data is readable and intact. If not, it will allow you to remove corrupt records. Note: This should be moved from manage.py to donkey command Usage: donkey tubcheck <tub_path> [--fix] Run on the host computer or the robot It will print summary of record count and channels recorded for each tub It will print the records that throw an exception while reading The optional --fix will delete records that have problems Histogram This command will show a pop-up window showing the histogram of record values in a given tub. Note: This should be moved from manage.py to donkey command Usage: donkey tubhist <tub_path> --rec=<\"user/angle\"> Run on the host computer When the --tub is omitted, it will check all tubs in the default data dir Plot Predictions This command allows you plot steering and throttle against predictions coming from a trained model. Note: This should be moved from manage.py to donkey command Usage: donkey tubplot <tub_path> [--model=<model_path>] This command may be run from ~/mycar dir Run on the host computer Will show a pop-up window showing the plot of steering values in a given tub compared to NN predictions from the trained model When the --tub is omitted, it will check all tubs in the default data dir Continuous Rsync This command uses rsync to copy files from your pi to your host. It does so in a loop, continuously copying files. By default, it will also delete any files on the host that are deleted on the pi. This allows your PS3 Triangle edits to affect the files on both machines. Usage: donkey consync [--dir = <data_path>] [--delete=<y|n>] Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . This command may be run from ~/mycar dir Continuous Train This command fires off the keras training in a mode where it will continuously look for new data at the end of every epoch. Usage: donkey contrain [--tub=<data_path>] [--model=<path to model>] [--transfer=<path to model>] [--type=<linear|categorical|rnn|imu|behavior|3d>] [--aug] This command may be run from ~/mycar dir Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . Optionally it can send the model file to your pi when it achieves a best loss. In config.py set SEND_BEST_MODEL_TO_PI = True . Your pi drive loop will autoload the weights file when it changes. This works best if car started with .json weights like: python manage.py drive --model models/drive.json Joystick Wizard This command line wizard will walk you through the steps to use your joystick. Usage: donkey createjs This command may be run from ~/mycar dir Run on the pi First make sure the OS can access your device. The utility jstest can be useful here. Installed via: sudo apt install joystick Debian commonly creates the joystick device file at /dev/input/js0 . If not, find out where. Run the command donkey createjs and it will create a file, by default my_joystick.py. Drop that next to your manage.py Modify manage.py to replace: from donkeycar.parts.controller import get_js_controller ctr = get_js_controller(cfg) with from my_joystick import MyJoystickController ctr = MyJoystickController(throttle_dir=cfg.JOYSTICK_THROTTLE_DIR, throttle_scale=cfg.JOYSTICK_MAX_THROTTLE, steering_scale=cfg.JOYSTICK_STEERING_SCALE, auto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE) ctr.set_deadzone(cfg.JOYSTICK_DEADZONE) Visualize CNN filter activations Shows feature maps of the provided image for each filter in each of the convolutional layers in the model provided. Debugging tool to visualize how well feature extraction is performing. Usage: donkey cnnactivations [--tub=<data_path>] [--model=<path to model>] This will open a figure for each Conv2d layer in the model. Example: donkey cnnactivations --model models/model.h5 --image data/tub/1_cam-image_array_.jpg","title":"Donkey Command-line Utilities"},{"location":"utility/donkey/#donkey-command-line-utilities","text":"The donkey command is created when you install the donkeycar Python package. This is a Python script that adds some important functionality. The operations here are vehicle independent, and should work on any hardware configuration.","title":"Donkey Command-line Utilities"},{"location":"utility/donkey/#create-car","text":"This command creates a new dir which will contain the files needed to run and train your robot. Usage: donkey createcar --path <dir> [--overwrite] [--template <donkey2>] This command may be run from any dir Run on the host computer or the robot It uses the --path as the destination dir to create. If .py files exist there, it will not overwrite them, unless the optional --overwrite is used. The optional --template will specify the template file to start from. For a list of templates, see the donkeycar/templates dir. This source template will be copied over the manage.py for the user.","title":"Create Car"},{"location":"utility/donkey/#find-car","text":"This command attempts to locate your car on the local network using nmap. Usage: donkey findcar Run on the host computer Prints the host computer IP address and the car IP address if found Requires the nmap utility: sudo apt install nmap","title":"Find Car"},{"location":"utility/donkey/#calibrate-car","text":"This command allows you to manually enter values to interactively set the PWM values and experiment with how your robot responds. See also more information. Usage: donkey calibrate --channel <0-15 channel id> Run on the host computer Opens the PWM channel specified by --channel Type integer values to specify PWM values and hit enter Hit Ctrl + C to exit","title":"Calibrate Car"},{"location":"utility/donkey/#clean-data-in-tub","text":"Opens a web server to delete bad data from a tub. Usage: donkey tubclean <folder containing tubs> Run on pi or host computer. Opens the web server to delete bad data. Hit Ctrl + C to exit","title":"Clean data in Tub"},{"location":"utility/donkey/#make-movie-from-tub","text":"This command allows you to create a movie file from the images in a Tub. Usage: donkey makemovie --tub=<tub_path> [--out=<tub_movie.mp4>] [--config=<config.py>] [--model=<model path>] [--model_type=(linear|categorical|rnn|imu|behavior|3d)] [--start=0] [--end=-1] [--scale=2] [--salient] Run on the host computer or the robot Uses the image records from --tub dir path given Creates a movie given by --out . Codec is inferred from file extension. Default: tub_movie.mp4 Optional argument to specify a different config.py other than default: config.py Optional model argument will load the keras model and display prediction as lines on the movie model_type may optionally give a hint about what model type we are loading. Categorical is default. optional --salient will overlay a visualization of which pixels excited the NN the most optional --start and/or --end can specify a range of frame numbers to use. scale will cause ouput image to be scaled by this amount","title":"Make Movie from Tub"},{"location":"utility/donkey/#check-tub","text":"This command allows you to see how many records are contained in any/all tubs. It will also open each record and ensure that the data is readable and intact. If not, it will allow you to remove corrupt records. Note: This should be moved from manage.py to donkey command Usage: donkey tubcheck <tub_path> [--fix] Run on the host computer or the robot It will print summary of record count and channels recorded for each tub It will print the records that throw an exception while reading The optional --fix will delete records that have problems","title":"Check Tub"},{"location":"utility/donkey/#histogram","text":"This command will show a pop-up window showing the histogram of record values in a given tub. Note: This should be moved from manage.py to donkey command Usage: donkey tubhist <tub_path> --rec=<\"user/angle\"> Run on the host computer When the --tub is omitted, it will check all tubs in the default data dir","title":"Histogram"},{"location":"utility/donkey/#plot-predictions","text":"This command allows you plot steering and throttle against predictions coming from a trained model. Note: This should be moved from manage.py to donkey command Usage: donkey tubplot <tub_path> [--model=<model_path>] This command may be run from ~/mycar dir Run on the host computer Will show a pop-up window showing the plot of steering values in a given tub compared to NN predictions from the trained model When the --tub is omitted, it will check all tubs in the default data dir","title":"Plot Predictions"},{"location":"utility/donkey/#continuous-rsync","text":"This command uses rsync to copy files from your pi to your host. It does so in a loop, continuously copying files. By default, it will also delete any files on the host that are deleted on the pi. This allows your PS3 Triangle edits to affect the files on both machines. Usage: donkey consync [--dir = <data_path>] [--delete=<y|n>] Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . This command may be run from ~/mycar dir","title":"Continuous Rsync"},{"location":"utility/donkey/#continuous-train","text":"This command fires off the keras training in a mode where it will continuously look for new data at the end of every epoch. Usage: donkey contrain [--tub=<data_path>] [--model=<path to model>] [--transfer=<path to model>] [--type=<linear|categorical|rnn|imu|behavior|3d>] [--aug] This command may be run from ~/mycar dir Run on the host computer First copy your public key to the pi so you don't need a password for each rsync: cat ~/.ssh/id_rsa.pub | ssh pi@<your pi ip> 'cat >> .ssh/authorized_keys' If you don't have a id_rsa.pub then google how to make one Edit your config.py and make sure the fields PI_USERNAME , PI_HOSTNAME , PI_DONKEY_ROOT are setup. Only on windows, you need to set PI_PASSWD . Optionally it can send the model file to your pi when it achieves a best loss. In config.py set SEND_BEST_MODEL_TO_PI = True . Your pi drive loop will autoload the weights file when it changes. This works best if car started with .json weights like: python manage.py drive --model models/drive.json","title":"Continuous Train"},{"location":"utility/donkey/#joystick-wizard","text":"This command line wizard will walk you through the steps to use your joystick. Usage: donkey createjs This command may be run from ~/mycar dir Run on the pi First make sure the OS can access your device. The utility jstest can be useful here. Installed via: sudo apt install joystick Debian commonly creates the joystick device file at /dev/input/js0 . If not, find out where. Run the command donkey createjs and it will create a file, by default my_joystick.py. Drop that next to your manage.py Modify manage.py to replace: from donkeycar.parts.controller import get_js_controller ctr = get_js_controller(cfg) with from my_joystick import MyJoystickController ctr = MyJoystickController(throttle_dir=cfg.JOYSTICK_THROTTLE_DIR, throttle_scale=cfg.JOYSTICK_MAX_THROTTLE, steering_scale=cfg.JOYSTICK_STEERING_SCALE, auto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE) ctr.set_deadzone(cfg.JOYSTICK_DEADZONE)","title":"Joystick Wizard"},{"location":"utility/donkey/#visualize-cnn-filter-activations","text":"Shows feature maps of the provided image for each filter in each of the convolutional layers in the model provided. Debugging tool to visualize how well feature extraction is performing. Usage: donkey cnnactivations [--tub=<data_path>] [--model=<path to model>] This will open a figure for each Conv2d layer in the model. Example: donkey cnnactivations --model models/model.h5 --image data/tub/1_cam-image_array_.jpg","title":"Visualize CNN filter activations"}]}